{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (606, 12)\n",
      "Testing set shape: (203, 12)\n",
      "\n",
      "----- Tuning SVM -----\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best CV Accuracy: 0.8680\n",
      "Test Set Accuracy: 0.8768\n",
      "Best Parameters: {'svc__C': 10, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}\n",
      "\n",
      "----- Tuning Random Forest -----\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best CV Accuracy: 0.9010\n",
      "Test Set Accuracy: 0.8916\n",
      "Best Parameters: {'rf__max_depth': 10, 'rf__n_estimators': 100}\n",
      "\n",
      "----- Tuning Gradient Boosting -----\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best CV Accuracy: 0.9010\n",
      "Test Set Accuracy: 0.9015\n",
      "Best Parameters: {'gb__learning_rate': 0.1, 'gb__max_depth': 5, 'gb__n_estimators': 100}\n",
      "\n",
      "----- Tuning AdaBoost -----\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best CV Accuracy: 0.8713\n",
      "Test Set Accuracy: 0.8768\n",
      "Best Parameters: {'ada__learning_rate': 1.0, 'ada__n_estimators': 200}\n",
      "\n",
      "----- Tuning XGBoost -----\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best CV Accuracy: 0.8911\n",
      "Test Set Accuracy: 0.8818\n",
      "Best Parameters: {'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "\n",
      "----- Tuning Bagging -----\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best CV Accuracy: 0.8994\n",
      "Test Set Accuracy: 0.8916\n",
      "Best Parameters: {'bag__n_estimators': 50}\n",
      "\n",
      "\n",
      "==================================================\n",
      "          MODEL EXPLORATION SUMMARY\n",
      "==================================================\n",
      "               Model  Best CV Accuracy  Test Set Accuracy  Test Set F1 Score                                                             Best Parameters\n",
      "2  Gradient Boosting          0.901003           0.901478           0.850746     {'gb__learning_rate': 0.1, 'gb__max_depth': 5, 'gb__n_estimators': 100}\n",
      "1      Random Forest          0.900989           0.891626           0.828125                              {'rf__max_depth': 10, 'rf__n_estimators': 100}\n",
      "5            Bagging          0.899377           0.891626           0.830769                                                   {'bag__n_estimators': 50}\n",
      "4            XGBoost          0.891085           0.881773           0.806452  {'xgb__learning_rate': 0.1, 'xgb__max_depth': 3, 'xgb__n_estimators': 100}\n",
      "0                SVM          0.867999           0.876847           0.793388              {'svc__C': 10, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}\n",
      "3           AdaBoost          0.871278           0.876847           0.789916                       {'ada__learning_rate': 1.0, 'ada__n_estimators': 200}\n",
      "\n",
      "\n",
      "Full summary saved to 'results/all_models_summary.csv'\n",
      "Configuration for the best model (Gradient Boosting) saved for the final step.\n"
     ]
    }
   ],
   "source": [
    "# 2_model_exploration [Target-Group].ipynb (Multi-Model Version)\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "# Import all the models you want to test\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the processed data from Notebook 1\n",
    "try:\n",
    "    df = pd.read_csv('processed_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'processed_data.csv' not found. Please run Notebook 1 first.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# --- Prepare Data for Modeling ---\n",
    "# Target variable is 'CDR' (Clinical Dementia Rating). We make it a binary target 'Group'.\n",
    "df['Group'] = (df['CDR'] > 0).astype(int)\n",
    "df.drop('CDR', axis=1, inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(['Group', 'Subject ID'], axis=1)\n",
    "y = df['Group']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# --- Define Models and Hyperparameter Grids ---\n",
    "# We will create a pipeline with scaling for each model\n",
    "\n",
    "# 1. Support Vector Machine (SVM)\n",
    "pipe_svc = Pipeline([('scaler', StandardScaler()), ('svc', SVC(probability=True, random_state=42))])\n",
    "param_svc = {'svc__C': [0.1, 1, 10], 'svc__gamma': ['scale', 'auto'], 'svc__kernel': ['rbf', 'linear']}\n",
    "\n",
    "# 2. Random Forest\n",
    "pipe_rf = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(random_state=42))])\n",
    "param_rf = {'rf__n_estimators': [100, 200], 'rf__max_depth': [10, 20, None]}\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "pipe_gb = Pipeline([('scaler', StandardScaler()), ('gb', GradientBoostingClassifier(random_state=42))])\n",
    "param_gb = {'gb__n_estimators': [100, 200], 'gb__learning_rate': [0.05, 0.1], 'gb__max_depth': [3, 5]}\n",
    "\n",
    "# 4. AdaBoost\n",
    "pipe_ada = Pipeline([('scaler', StandardScaler()), ('ada', AdaBoostClassifier(random_state=42))])\n",
    "param_ada = {'ada__n_estimators': [50, 100, 200], 'ada__learning_rate': [0.05, 0.1, 1.0]}\n",
    "\n",
    "# 5. XGBoost\n",
    "pipe_xgb = Pipeline([('scaler', StandardScaler()), ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))])\n",
    "param_xgb = {'xgb__n_estimators': [100, 200], 'xgb__learning_rate': [0.05, 0.1], 'xgb__max_depth': [3, 5]}\n",
    "\n",
    "# 6. Bagging Classifier\n",
    "pipe_bag = Pipeline([('scaler', StandardScaler()), ('bag', BaggingClassifier(random_state=42))])\n",
    "param_bag = {'bag__n_estimators': [50, 100, 200]}\n",
    "\n",
    "\n",
    "# --- Run GridSearchCV for Each Model ---\n",
    "pipelines = [pipe_svc, pipe_rf, pipe_gb, pipe_ada, pipe_xgb, pipe_bag]\n",
    "params = [param_svc, param_rf, param_gb, param_ada, param_xgb, param_bag]\n",
    "model_names = ['SVM', 'Random Forest', 'Gradient Boosting', 'AdaBoost', 'XGBoost', 'Bagging']\n",
    "all_results = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, pipe in enumerate(pipelines):\n",
    "    print(f\"\\n----- Tuning {model_names[i]} -----\")\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=params[i], cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Best CV Accuracy: {best_score:.4f}\")\n",
    "    print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Store results for final report\n",
    "    result = {\n",
    "        'Model': model_names[i],\n",
    "        'Best CV Accuracy': best_score,\n",
    "        'Test Set Accuracy': test_accuracy,\n",
    "        'Test Set F1 Score': test_f1,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "    all_results.append(result)\n",
    "\n",
    "# --- Save and Display Results ---\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values(by='Test Set Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"          MODEL EXPLORATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Save the detailed results for your research paper\n",
    "results_df.to_csv('results/all_models_summary.csv', index=False)\n",
    "print(\"\\n\\nFull summary saved to 'results/all_models_summary.csv'\")\n",
    "\n",
    "# Save the best model's configuration for Notebook 3\n",
    "best_model_config = results_df.iloc[0]\n",
    "best_model_config.to_json('results/best_model_config.json')\n",
    "print(f\"Configuration for the best model ({best_model_config['Model']}) saved for the final step.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
