{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96cb175b",
   "metadata": {},
   "source": [
    "# 2. Model Exploration and Hyperparameter Tuning\n",
    "\n",
    "## üìù Overview\n",
    "This notebook is the second step in the dementia prediction pipeline. Its purpose is to:\n",
    "1.  **Load** the pre-cleaned and split data from `1_dataset_analysis.ipynb`.\n",
    "2.  Define a **preprocessing pipeline** to handle scaling and encoding.\n",
    "3.  Use **SMOTE** to address class imbalance in the training data.\n",
    "4.  Train a variety of machine learning models using **GridSearchCV** to find the best hyperparameters for each.\n",
    "5.  **Save** the trained models and their performance results for the final implementation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46073c67",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb398a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af20dae",
   "metadata": {},
   "source": [
    "## ML Model Results Storage Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a95e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results storage framework loaded successfully!\n",
      "This framework will save results, plots, and models to the 'ModelExploration' directory.\n"
     ]
    }
   ],
   "source": [
    "# ML Model Result Storage Framework\n",
    "precision = []\n",
    "roc_auc = []\n",
    "\n",
    "# Function to call for storing the results\n",
    "def store_results(model, config, acc, f1_score, rec, prec, roc):\n",
    "    \"\"\"\n",
    "    Store model performance results.\n",
    "    \"\"\"\n",
    "    ML_Model.append(model)\n",
    "    ML_Config.append(config)\n",
    "    accuracy.append(round(acc, 6))\n",
    "    f1.append(round(f1_score, 6))\n",
    "    recall.append(round(rec, 6))\n",
    "    precision.append(round(prec, 6))\n",
    "    roc_auc.append(round(roc, 6))\n",
    "\n",
    "# Function to display and save results\n",
    "def display_and_save_results(filename_prefix='model_exploration'):\n",
    "    \"\"\"\n",
    "    Create dataframe from results, display, and save to CSV in the 'AnalysisMain/results' directory.\n",
    "    \"\"\"\n",
    "    # Creating the dataframe\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    \n",
    "    # Remove duplicates if any\n",
    "    result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL PERFORMANCE RESULTS\")\n",
    "    print(\"=\"*100)\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Saving the result to a CSV file\n",
    "    save_path = os.path.join(results_dir, f'{filename_prefix}_results.csv')\n",
    "    result.to_csv(save_path, index=False)\n",
    "    print(f\"\\nResults saved to {save_path}\")\n",
    "    \n",
    "    # Sorting the dataframe on F1 Score and Accuracy\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SORTED MODEL PERFORMANCE RESULTS (by F1 Score and Accuracy)\")\n",
    "    print(\"=\"*100)\n",
    "    print(sorted_result.to_string(index=False))\n",
    "    \n",
    "    # Saving the sorted result to a CSV file\n",
    "    sorted_save_path = os.path.join(results_dir, f'sorted_{filename_prefix}_results.csv')\n",
    "    sorted_result.to_csv(sorted_save_path, index=False)\n",
    "    print(f\"\\nSorted results saved to {sorted_save_path}\")\n",
    "    \n",
    "    return result, sorted_result\n",
    "\n",
    "# Function to clear results\n",
    "def clear_results():\n",
    "    \"\"\"Clear all stored results.\"\"\"\n",
    "    global ML_Model, ML_Config, accuracy, f1, recall, precision, roc_auc\n",
    "    ML_Model.clear()\n",
    "    ML_Config.clear()\n",
    "    accuracy.clear()\n",
    "    f1.clear()\n",
    "    recall.clear()\n",
    "    precision.clear()\n",
    "    roc_auc.clear()\n",
    "    print(\"Results cleared!\")\n",
    "\n",
    "# Function to plot model comparison\n",
    "def plot_model_comparison(result_df, plot_filename=\"model_performance_comparison.png\"):\n",
    "    \"\"\"\n",
    "    Create visualization comparing model performances and save to 'AnalysisMain/plots'.\n",
    "    \"\"\"\n",
    "    # Convert scores to percentages for plotting\n",
    "    metrics_cols = ['Accuracy', 'F1 Score', 'Recall', 'Precision', 'ROC_AUC']\n",
    "    plot_df = result_df.copy()\n",
    "    \n",
    "    for col in metrics_cols:\n",
    "        plot_df[col] = plot_df[col] * 100\n",
    "    \n",
    "    # Create subplot for each metric\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_cols):\n",
    "        # Group by model and get mean performance across configurations\n",
    "        model_performance = plot_df.groupby('ML Model')[metric].mean().sort_values(ascending=False)\n",
    "        \n",
    "        # Create bar plot\n",
    "        ax = axes[idx]\n",
    "        bars = sns.barplot(x=model_performance.index, y=model_performance.values, ax=ax, palette='Blues_r')\n",
    "        \n",
    "        ax.set_title(f'Average {metric}', fontweight='bold')\n",
    "        ax.set_ylabel(f'{metric} (%)')\n",
    "        ax.set_xlabel('')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(axis='y', alpha=0.5)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars.patches:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Hide the last subplot if we have 5 metrics\n",
    "    if len(metrics_cols) < 6:\n",
    "        axes[5].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison', fontsize=20, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = os.path.join(plots_dir, plot_filename)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Comparison plot saved to: {save_path}\")\n",
    "\n",
    "print(\"Model results storage framework loaded successfully!\")\n",
    "print(\"This framework will save results, plots, and models to the 'ModelExploration' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8ff57",
   "metadata": {},
   "source": [
    "# Define Preprocessing Pipeline\n",
    "\n",
    "Before training the models, we need to create a preprocessing pipeline. This pipeline will handle:\n",
    "1.  **Scaling**: Applying `StandardScaler` to all numerical features to standardize their range.\n",
    "2.  **Encoding**: Applying `OneHotEncoder` to the categorical feature (`M/F`) to convert it into a numerical format.\n",
    "\n",
    "We use a `ColumnTransformer` to apply these different transformations to the correct columns. This ensures that the same steps are consistently applied during both training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28c6989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from 'processed_data' directory!\n",
      "--------------------------------------------------\n",
      "X_train shape: (647, 12)\n",
      "X_val shape: (162, 12)\n",
      "\n",
      "Training target distribution:\n",
      "Group\n",
      "Nondemented    0.774343\n",
      "Demented       0.225657\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation target distribution:\n",
      "Group\n",
      "Nondemented    0.771605\n",
      "Demented       0.228395\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "First 5 rows of X_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M/F</th>\n",
       "      <th>Age</th>\n",
       "      <th>Educ</th>\n",
       "      <th>SES</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>CDR</th>\n",
       "      <th>eTIV</th>\n",
       "      <th>nWBV</th>\n",
       "      <th>ASF</th>\n",
       "      <th>Delay</th>\n",
       "      <th>Visit</th>\n",
       "      <th>MR Delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>M</td>\n",
       "      <td>84</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1550</td>\n",
       "      <td>0.665</td>\n",
       "      <td>1.132</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>621.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>M</td>\n",
       "      <td>75</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1534</td>\n",
       "      <td>0.771</td>\n",
       "      <td>1.144</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>F</td>\n",
       "      <td>69</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380</td>\n",
       "      <td>0.809</td>\n",
       "      <td>1.272</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>F</td>\n",
       "      <td>80</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1323</td>\n",
       "      <td>0.738</td>\n",
       "      <td>1.326</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1385</td>\n",
       "      <td>0.819</td>\n",
       "      <td>1.267</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    M/F  Age  Educ  SES  MMSE  CDR  eTIV   nWBV    ASF  Delay  Visit  MR Delay\n",
       "764   M   84  14.0  2.0  22.0  0.5  1550  0.665  1.132    2.0    2.0     621.0\n",
       "213   M   75   5.0  2.0  29.0  0.0  1534  0.771  1.144    2.0    1.0       0.0\n",
       "382   F   69   4.0  3.0  29.0  0.0  1380  0.809  1.272    2.0    1.0       0.0\n",
       "456   F   80  16.0  2.0  29.0  0.0  1323  0.738  1.326    2.0    2.0     730.0\n",
       "393   F   50  12.0  2.0  30.0  0.0  1385  0.819  1.267    2.0    1.0       0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Preprocessing Pipeline\n",
    "\n",
    "# Define the directory where the processed data was saved from the previous notebook\n",
    "processed_data_dir = 'Analysis/processed_data'\n",
    "\n",
    "# Load the training and validation sets\n",
    "X_train = joblib.load(os.path.join(processed_data_dir, 'X_train.joblib'))\n",
    "X_val = joblib.load(os.path.join(processed_data_dir, 'X_val.joblib'))\n",
    "y_train = joblib.load(os.path.join(processed_data_dir, 'y_train.joblib'))\n",
    "y_val = joblib.load(os.path.join(processed_data_dir, 'y_val.joblib'))\n",
    "\n",
    "print(\"Data loaded successfully from 'processed_data' directory!\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"\\nTraining target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\nValidation target distribution:\\n{y_val.value_counts(normalize=True)}\")\n",
    "\n",
    "# Display the first few rows of the training data to confirm\n",
    "print(\"\\nFirst 5 rows of X_train:\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c849cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892602bb",
   "metadata": {},
   "source": [
    "### SVM with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Delay', 'Visit', 'MR Delay']\n",
      "Categorical features: ['M/F']\n",
      "\n",
      "Preprocessed data shape: (647, 13)\n",
      "All features are now numeric: True\n",
      "Initialized result storage lists.\n",
      "\n",
      "=== START: SVM Configuration Sweep with Custom Hyperparameters ===\n",
      "\n",
      "‚úì Configuration 1: Preprocessed Data\n",
      "‚úì Configuration 2: Normalized Data (MinMax)\n",
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features: 10\n",
      "‚úì Configuration 3: SelectKBest\n",
      "\n",
      "=== RFECV Feature Selection ===\n",
      "Optimal number of features by RFECV: 3\n",
      "‚úì Configuration 4: RFECV\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components for 90.0% variance: 3\n",
      "‚úì Configuration 5: PCA\n",
      "‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\n",
      "‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\n",
      "\n",
      "=== Adding Undersampling Configurations ===\n",
      "‚úì Configuration 8: RandomUnderSampler (Undersampling)\n",
      "‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\n",
      "‚úì Configuration 10: NearMiss (Undersampling - selective)\n",
      "‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\n",
      "\n",
      "Total configurations: 11\n",
      "\n",
      "================================================================================\n",
      "RUNNING SVM WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Configuration: Preprocessed Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Preprocessed Data':\n",
      "  C: [0.01, 0.1, 1, 10]\n",
      "  gamma: ['scale', 'auto', 0.001, 0.01]\n",
      "  kernel: ['rbf', 'linear']\n",
      "  degree: [2]\n",
      "  coef0: [0.0]\n",
      "Total combinations: 32\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.979907  0.970299 0.955479   0.987354 0.994518\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0016\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 10\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.958037\n",
      "\n",
      "================================================================================\n",
      "Configuration: Normalized Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Normalized Data':\n",
      "  C: [0.1, 1, 10, 100]\n",
      "  gamma: ['scale', 0.01, 0.1]\n",
      "  kernel: ['poly', 'rbf']\n",
      "  degree: [2, 3, 4]\n",
      "  coef0: [0.0, 0.5, 1.0]\n",
      "Total combinations: 216\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.983321\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 1\n",
      "  coef0: 0.0\n",
      "  degree: 3\n",
      "  gamma: scale\n",
      "  kernel: poly\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: SelectKBest\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SelectKBest':\n",
      "  C: [0.1, 1, 10]\n",
      "  gamma: ['scale', 'auto', 0.01]\n",
      "  kernel: ['rbf', 'poly', 'linear']\n",
      "  degree: [2, 3]\n",
      "  coef0: [0.0, 0.1]\n",
      "Total combinations: 108\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.989364\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 10\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.958034\n",
      "\n",
      "================================================================================\n",
      "Configuration: RFECV\n",
      "================================================================================\n",
      "Hyperparameter grid for 'RFECV':\n",
      "  C: [0.5, 1, 5, 10]\n",
      "  gamma: ['scale', 0.001, 0.01]\n",
      "  kernel: ['rbf', 'sigmoid']\n",
      "  degree: [2, 3]\n",
      "  coef0: [0.0, 0.5]\n",
      "Total combinations: 96\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.968747\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 1\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.957822\n",
      "\n",
      "================================================================================\n",
      "Configuration: PCA\n",
      "================================================================================\n",
      "Hyperparameter grid for 'PCA':\n",
      "  C: [0.1, 1, 10, 50]\n",
      "  gamma: ['scale', 'auto', 0.01]\n",
      "  kernel: ['linear', 'sigmoid', 'rbf']\n",
      "  degree: [2]\n",
      "  coef0: [0.0, 0.5, 1.0]\n",
      "Total combinations: 108\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.978454\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 50\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: auto\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + StandardScaler\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + StandardScaler':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.993711\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0217\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 3\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956065\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + GridSearchCV\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + GridSearchCV':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.993711\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0217\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 3\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956065\n",
      "\n",
      "================================================================================\n",
      "Configuration: RandomUnderSampler\n",
      "================================================================================\n",
      "Hyperparameter grid for 'RandomUnderSampler':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.970634  0.958086 0.959198   0.956986 0.984292\n",
      "    Test  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0047\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 1\n",
      "  model__coef0: 0.0\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956630\n",
      "\n",
      "================================================================================\n",
      "Configuration: TomekLinks\n",
      "================================================================================\n",
      "Hyperparameter grid for 'TomekLinks':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.981134\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 2\n",
      "  model__gamma: scale\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: NearMiss\n",
      "================================================================================\n",
      "Hyperparameter grid for 'NearMiss':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.970634  0.956363 0.939785   0.975894 0.983430\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0108\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 100\n",
      "  model__coef0: 0.0\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.01\n",
      "  model__kernel: rbf\n",
      "\n",
      "Best CV score: 0.947437\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + Tomek\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + Tomek':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.990526\n",
      "    Test  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0031\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.954139\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SVM evaluation complete for all configurations.\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "MODEL PERFORMANCE RESULTS\n",
      "====================================================================================================\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üìà Final Results:\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üèÜ Sorted Results (by F1 Score):\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üìä Performance by Sampling Technique:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "No Sampling:\n",
      "  Avg Accuracy: 0.981481\n",
      "  Avg F1 Score: 0.973976\n",
      "  Avg ROC-AUC: 0.988411\n",
      "  Best Config: Preprocessed Data\n",
      "\n",
      "Oversampling:\n",
      "  Avg Accuracy: 0.987654\n",
      "  Avg F1 Score: 0.982810\n",
      "  Avg ROC-AUC: 0.996973\n",
      "  Best Config: SMOTE + StandardScaler\n",
      "\n",
      "Undersampling:\n",
      "  Avg Accuracy: 0.979424\n",
      "  Avg F1 Score: 0.971191\n",
      "  Avg ROC-AUC: 0.988613\n",
      "  Best Config: TomekLinks\n",
      "\n",
      "Combined:\n",
      "  Avg Accuracy: 0.969136\n",
      "  Avg F1 Score: 0.957411\n",
      "  Avg ROC-AUC: 0.996324\n",
      "  Best Config: SMOTE + Tomek\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SVM with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: SVM Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Focus on RBF and linear kernels\n",
    "param_grid_1 = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'degree': [2],  # Not used for rbf/linear but required\n",
    "    'coef0': [0.0]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Explore polynomial kernels\n",
    "param_grid_2 = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.01, 0.1],\n",
    "    'kernel': ['poly', 'rbf'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - Focus on simpler models\n",
    "param_grid_3 = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    'kernel': ['rbf', 'poly', 'linear'],\n",
    "    'degree': [2, 3],\n",
    "    'coef0': [0.0, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Similar to SelectKBest but different ranges\n",
    "param_grid_4 = {\n",
    "    'C': [0.5, 1, 5, 10],\n",
    "    'gamma': ['scale', 0.001, 0.01],\n",
    "    'kernel': ['rbf', 'sigmoid'],\n",
    "    'degree': [2, 3],\n",
    "    'coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Focus on linear and sigmoid\n",
    "param_grid_5 = {\n",
    "    'C': [0.1, 1, 10, 50],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    'kernel': ['linear', 'sigmoid', 'rbf'],\n",
    "    'degree': [2],\n",
    "    'coef0': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Balanced approach (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__gamma': ['scale', 0.001, 0.01, 0.1],\n",
    "    'model__kernel': ['rbf', 'poly', 'linear'],\n",
    "    'model__degree': [2, 3],\n",
    "    'model__coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__gamma': ['scale', 0.001, 0.01, 0.1],\n",
    "    'model__kernel': ['rbf', 'poly', 'linear'],\n",
    "    'model__degree': [2, 3],\n",
    "    'model__coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        SVC(kernel='linear', random_state=RANDOM_STATE), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "svm_estimator = SVC(kernel='linear', random_state=RANDOM_STATE)\n",
    "rfecv = RFECV(\n",
    "    estimator=svm_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=svm_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run SVM with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING SVM WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                SVC(probability=True, random_state=RANDOM_STATE), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä Support Vector Machine Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Good generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'SVM',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('SVM')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SVM evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('svm_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50abe0",
   "metadata": {},
   "source": [
    "### SVM with PCA 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61756ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Delay', 'Visit', 'MR Delay']\n",
      "Categorical features: ['M/F']\n",
      "\n",
      "Preprocessed data shape: (647, 13)\n",
      "All features are now numeric: True\n",
      "Results cleared!\n",
      "\n",
      "=== START: SVM Configuration Sweep with Custom Hyperparameters ===\n",
      "\n",
      "‚úì Configuration 1: Preprocessed Data\n",
      "‚úì Configuration 2: Normalized Data (MinMax)\n",
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features: 10\n",
      "‚úì Configuration 3: SelectKBest\n",
      "\n",
      "=== RFECV Feature Selection ===\n",
      "Optimal number of features by RFECV: 3\n",
      "‚úì Configuration 4: RFECV\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components for 95.0% variance: 3\n",
      "‚úì Configuration 5: PCA\n",
      "‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\n",
      "‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\n",
      "\n",
      "=== Adding Undersampling Configurations ===\n",
      "‚úì Configuration 8: RandomUnderSampler (Undersampling)\n",
      "‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\n",
      "‚úì Configuration 10: NearMiss (Undersampling - selective)\n",
      "‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\n",
      "\n",
      "Total configurations: 11\n",
      "\n",
      "================================================================================\n",
      "RUNNING SVM WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Configuration: Preprocessed Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Preprocessed Data':\n",
      "  C: [0.01, 0.1, 1, 10]\n",
      "  gamma: ['scale', 'auto', 0.001, 0.01]\n",
      "  kernel: ['rbf', 'linear']\n",
      "  degree: [2]\n",
      "  coef0: [0.0]\n",
      "Total combinations: 32\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.979907  0.970299 0.955479   0.987354 0.994518\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0016\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 10\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.958037\n",
      "\n",
      "================================================================================\n",
      "Configuration: Normalized Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Normalized Data':\n",
      "  C: [0.1, 1, 10, 100]\n",
      "  gamma: ['scale', 0.01, 0.1]\n",
      "  kernel: ['poly', 'rbf']\n",
      "  degree: [2, 3, 4]\n",
      "  coef0: [0.0, 0.5, 1.0]\n",
      "Total combinations: 216\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.983321\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 1\n",
      "  coef0: 0.0\n",
      "  degree: 3\n",
      "  gamma: scale\n",
      "  kernel: poly\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: SelectKBest\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SelectKBest':\n",
      "  C: [0.1, 1, 10]\n",
      "  gamma: ['scale', 'auto', 0.01]\n",
      "  kernel: ['rbf', 'poly', 'linear']\n",
      "  degree: [2, 3]\n",
      "  coef0: [0.0, 0.1]\n",
      "Total combinations: 108\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.989364\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 10\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.958034\n",
      "\n",
      "================================================================================\n",
      "Configuration: RFECV\n",
      "================================================================================\n",
      "Hyperparameter grid for 'RFECV':\n",
      "  C: [0.5, 1, 5, 10]\n",
      "  gamma: ['scale', 0.001, 0.01]\n",
      "  kernel: ['rbf', 'sigmoid']\n",
      "  degree: [2, 3]\n",
      "  coef0: [0.0, 0.5]\n",
      "Total combinations: 96\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.968747\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 1\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.957822\n",
      "\n",
      "================================================================================\n",
      "Configuration: PCA\n",
      "================================================================================\n",
      "Hyperparameter grid for 'PCA':\n",
      "  C: [0.1, 1, 10, 50]\n",
      "  gamma: ['scale', 'auto', 0.01]\n",
      "  kernel: ['linear', 'sigmoid', 'rbf']\n",
      "  degree: [2]\n",
      "  coef0: [0.0, 0.5, 1.0]\n",
      "Total combinations: 108\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.978454\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 50\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: auto\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + StandardScaler\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + StandardScaler':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.993711\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0217\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 3\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956065\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + GridSearchCV\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + GridSearchCV':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.993711\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0217\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 3\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956065\n",
      "\n",
      "================================================================================\n",
      "Configuration: RandomUnderSampler\n",
      "================================================================================\n",
      "Hyperparameter grid for 'RandomUnderSampler':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.970634  0.958086 0.959198   0.956986 0.984292\n",
      "    Test  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0047\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 1\n",
      "  model__coef0: 0.0\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956630\n",
      "\n",
      "================================================================================\n",
      "Configuration: TomekLinks\n",
      "================================================================================\n",
      "Hyperparameter grid for 'TomekLinks':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.981134\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 2\n",
      "  model__gamma: scale\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: NearMiss\n",
      "================================================================================\n",
      "Hyperparameter grid for 'NearMiss':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.970634  0.956363 0.939785   0.975894 0.983430\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0108\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 100\n",
      "  model__coef0: 0.0\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.01\n",
      "  model__kernel: rbf\n",
      "\n",
      "Best CV score: 0.947437\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + Tomek\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + Tomek':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.990526\n",
      "    Test  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0031\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.954139\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SVM evaluation complete for all configurations.\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "MODEL PERFORMANCE RESULTS\n",
      "====================================================================================================\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üìà Final Results:\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üèÜ Sorted Results (by F1 Score):\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üìä Performance by Sampling Technique:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "No Sampling:\n",
      "  Avg Accuracy: 0.981481\n",
      "  Avg F1 Score: 0.973976\n",
      "  Avg ROC-AUC: 0.988411\n",
      "  Best Config: Preprocessed Data\n",
      "\n",
      "Oversampling:\n",
      "  Avg Accuracy: 0.987654\n",
      "  Avg F1 Score: 0.982810\n",
      "  Avg ROC-AUC: 0.996973\n",
      "  Best Config: SMOTE + StandardScaler\n",
      "\n",
      "Undersampling:\n",
      "  Avg Accuracy: 0.979424\n",
      "  Avg F1 Score: 0.971191\n",
      "  Avg ROC-AUC: 0.988613\n",
      "  Best Config: TomekLinks\n",
      "\n",
      "Combined:\n",
      "  Avg Accuracy: 0.969136\n",
      "  Avg F1 Score: 0.957411\n",
      "  Avg ROC-AUC: 0.996324\n",
      "  Best Config: SMOTE + Tomek\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SVM with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: SVM Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Focus on RBF and linear kernels\n",
    "param_grid_1 = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'degree': [2],  # Not used for rbf/linear but required\n",
    "    'coef0': [0.0]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Explore polynomial kernels\n",
    "param_grid_2 = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.01, 0.1],\n",
    "    'kernel': ['poly', 'rbf'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - Focus on simpler models\n",
    "param_grid_3 = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    'kernel': ['rbf', 'poly', 'linear'],\n",
    "    'degree': [2, 3],\n",
    "    'coef0': [0.0, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Similar to SelectKBest but different ranges\n",
    "param_grid_4 = {\n",
    "    'C': [0.5, 1, 5, 10],\n",
    "    'gamma': ['scale', 0.001, 0.01],\n",
    "    'kernel': ['rbf', 'sigmoid'],\n",
    "    'degree': [2, 3],\n",
    "    'coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Focus on linear and sigmoid\n",
    "param_grid_5 = {\n",
    "    'C': [0.1, 1, 10, 50],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    'kernel': ['linear', 'sigmoid', 'rbf'],\n",
    "    'degree': [2],\n",
    "    'coef0': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Balanced approach (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__gamma': ['scale', 0.001, 0.01, 0.1],\n",
    "    'model__kernel': ['rbf', 'poly', 'linear'],\n",
    "    'model__degree': [2, 3],\n",
    "    'model__coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__gamma': ['scale', 0.001, 0.01, 0.1],\n",
    "    'model__kernel': ['rbf', 'poly', 'linear'],\n",
    "    'model__degree': [2, 3],\n",
    "    'model__coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        SVC(kernel='linear', random_state=RANDOM_STATE), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "svm_estimator = SVC(kernel='linear', random_state=RANDOM_STATE)\n",
    "rfecv = RFECV(\n",
    "    estimator=svm_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=svm_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.95\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run SVM with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING SVM WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                SVC(probability=True, random_state=RANDOM_STATE), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä Support Vector Machine Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Good generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'SVM',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('SVM')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SVM evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('svm_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822a9fe",
   "metadata": {},
   "source": [
    "### SVM with PCA 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ea0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Delay', 'Visit', 'MR Delay']\n",
      "Categorical features: ['M/F']\n",
      "\n",
      "Preprocessed data shape: (647, 13)\n",
      "All features are now numeric: True\n",
      "Results cleared!\n",
      "\n",
      "=== START: SVM Configuration Sweep with Custom Hyperparameters ===\n",
      "\n",
      "‚úì Configuration 1: Preprocessed Data\n",
      "‚úì Configuration 2: Normalized Data (MinMax)\n",
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features: 10\n",
      "‚úì Configuration 3: SelectKBest\n",
      "\n",
      "=== RFECV Feature Selection ===\n",
      "Optimal number of features by RFECV: 3\n",
      "‚úì Configuration 4: RFECV\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components for 99.0% variance: 3\n",
      "‚úì Configuration 5: PCA\n",
      "‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\n",
      "‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\n",
      "\n",
      "=== Adding Undersampling Configurations ===\n",
      "‚úì Configuration 8: RandomUnderSampler (Undersampling)\n",
      "‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\n",
      "‚úì Configuration 10: NearMiss (Undersampling - selective)\n",
      "‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\n",
      "\n",
      "Total configurations: 11\n",
      "\n",
      "================================================================================\n",
      "RUNNING SVM WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Configuration: Preprocessed Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Preprocessed Data':\n",
      "  C: [0.01, 0.1, 1, 10]\n",
      "  gamma: ['scale', 'auto', 0.001, 0.01]\n",
      "  kernel: ['rbf', 'linear']\n",
      "  degree: [2]\n",
      "  coef0: [0.0]\n",
      "Total combinations: 32\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.979907  0.970299 0.955479   0.987354 0.994518\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0016\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 10\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.958037\n",
      "\n",
      "================================================================================\n",
      "Configuration: Normalized Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Normalized Data':\n",
      "  C: [0.1, 1, 10, 100]\n",
      "  gamma: ['scale', 0.01, 0.1]\n",
      "  kernel: ['poly', 'rbf']\n",
      "  degree: [2, 3, 4]\n",
      "  coef0: [0.0, 0.5, 1.0]\n",
      "Total combinations: 216\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.983321\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 1\n",
      "  coef0: 0.0\n",
      "  degree: 3\n",
      "  gamma: scale\n",
      "  kernel: poly\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: SelectKBest\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SelectKBest':\n",
      "  C: [0.1, 1, 10]\n",
      "  gamma: ['scale', 'auto', 0.01]\n",
      "  kernel: ['rbf', 'poly', 'linear']\n",
      "  degree: [2, 3]\n",
      "  coef0: [0.0, 0.1]\n",
      "Total combinations: 108\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.989364\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 10\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.958034\n",
      "\n",
      "================================================================================\n",
      "Configuration: RFECV\n",
      "================================================================================\n",
      "Hyperparameter grid for 'RFECV':\n",
      "  C: [0.5, 1, 5, 10]\n",
      "  gamma: ['scale', 0.001, 0.01]\n",
      "  kernel: ['rbf', 'sigmoid']\n",
      "  degree: [2, 3]\n",
      "  coef0: [0.0, 0.5]\n",
      "Total combinations: 96\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.968747\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 1\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.957822\n",
      "\n",
      "================================================================================\n",
      "Configuration: PCA\n",
      "================================================================================\n",
      "Hyperparameter grid for 'PCA':\n",
      "  C: [0.1, 1, 10, 50]\n",
      "  gamma: ['scale', 'auto', 0.01]\n",
      "  kernel: ['linear', 'sigmoid', 'rbf']\n",
      "  degree: [2]\n",
      "  coef0: [0.0, 0.5, 1.0]\n",
      "Total combinations: 108\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.978454\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  C: 50\n",
      "  coef0: 0.0\n",
      "  degree: 2\n",
      "  gamma: auto\n",
      "  kernel: rbf\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + StandardScaler\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + StandardScaler':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.993711\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0217\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 3\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956065\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + GridSearchCV\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + GridSearchCV':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.993711\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0217\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 3\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956065\n",
      "\n",
      "================================================================================\n",
      "Configuration: RandomUnderSampler\n",
      "================================================================================\n",
      "Hyperparameter grid for 'RandomUnderSampler':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.970634  0.958086 0.959198   0.956986 0.984292\n",
      "    Test  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0047\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 1\n",
      "  model__coef0: 0.0\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.956630\n",
      "\n",
      "================================================================================\n",
      "Configuration: TomekLinks\n",
      "================================================================================\n",
      "Hyperparameter grid for 'TomekLinks':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.981134\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 2\n",
      "  model__gamma: scale\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: NearMiss\n",
      "================================================================================\n",
      "Hyperparameter grid for 'NearMiss':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.970634  0.956363 0.939785   0.975894 0.983430\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0108\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 100\n",
      "  model__coef0: 0.0\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.01\n",
      "  model__kernel: rbf\n",
      "\n",
      "Best CV score: 0.947437\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + Tomek\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + Tomek':\n",
      "  model__C: [0.1, 1, 10, 100]\n",
      "  model__gamma: ['scale', 0.001, 0.01, 0.1]\n",
      "  model__kernel: ['rbf', 'poly', 'linear']\n",
      "  model__degree: [2, 3]\n",
      "  model__coef0: [0.0, 0.5]\n",
      "Total combinations: 192\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "üìä Support Vector Machine Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.965997  0.950871 0.946497   0.955439 0.990526\n",
      "    Test  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0031\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__C: 0.1\n",
      "  model__coef0: 0.5\n",
      "  model__degree: 2\n",
      "  model__gamma: 0.1\n",
      "  model__kernel: poly\n",
      "\n",
      "Best CV score: 0.954139\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SVM evaluation complete for all configurations.\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "MODEL PERFORMANCE RESULTS\n",
      "====================================================================================================\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üìà Final Results:\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üèÜ Sorted Results (by F1 Score):\n",
      "ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "     SVM SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996973\n",
      "     SVM      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.995676\n",
      "     SVM        Normalized Data  0.981481  0.973976 0.978486   0.969652 0.981622\n",
      "     SVM            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.987027\n",
      "     SVM                  RFECV  0.981481  0.973976 0.978486   0.969652 0.988973\n",
      "     SVM                    PCA  0.981481  0.973976 0.978486   0.969652 0.988757\n",
      "     SVM             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.987459\n",
      "     SVM               NearMiss  0.981481  0.973976 0.978486   0.969652 0.988541\n",
      "     SVM     RandomUnderSampler  0.975309  0.965620 0.974486   0.957473 0.989838\n",
      "     SVM          SMOTE + Tomek  0.969136  0.957411 0.970486   0.945902 0.996324\n",
      "\n",
      "üìä Performance by Sampling Technique:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "No Sampling:\n",
      "  Avg Accuracy: 0.981481\n",
      "  Avg F1 Score: 0.973976\n",
      "  Avg ROC-AUC: 0.988411\n",
      "  Best Config: Preprocessed Data\n",
      "\n",
      "Oversampling:\n",
      "  Avg Accuracy: 0.987654\n",
      "  Avg F1 Score: 0.982810\n",
      "  Avg ROC-AUC: 0.996973\n",
      "  Best Config: SMOTE + StandardScaler\n",
      "\n",
      "Undersampling:\n",
      "  Avg Accuracy: 0.979424\n",
      "  Avg F1 Score: 0.971191\n",
      "  Avg ROC-AUC: 0.988613\n",
      "  Best Config: TomekLinks\n",
      "\n",
      "Combined:\n",
      "  Avg Accuracy: 0.969136\n",
      "  Avg F1 Score: 0.957411\n",
      "  Avg ROC-AUC: 0.996324\n",
      "  Best Config: SMOTE + Tomek\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SVM with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: SVM Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Focus on RBF and linear kernels\n",
    "param_grid_1 = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'degree': [2],  # Not used for rbf/linear but required\n",
    "    'coef0': [0.0]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Explore polynomial kernels\n",
    "param_grid_2 = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.01, 0.1],\n",
    "    'kernel': ['poly', 'rbf'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - Focus on simpler models\n",
    "param_grid_3 = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    'kernel': ['rbf', 'poly', 'linear'],\n",
    "    'degree': [2, 3],\n",
    "    'coef0': [0.0, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Similar to SelectKBest but different ranges\n",
    "param_grid_4 = {\n",
    "    'C': [0.5, 1, 5, 10],\n",
    "    'gamma': ['scale', 0.001, 0.01],\n",
    "    'kernel': ['rbf', 'sigmoid'],\n",
    "    'degree': [2, 3],\n",
    "    'coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Focus on linear and sigmoid\n",
    "param_grid_5 = {\n",
    "    'C': [0.1, 1, 10, 50],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    'kernel': ['linear', 'sigmoid', 'rbf'],\n",
    "    'degree': [2],\n",
    "    'coef0': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Balanced approach (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__gamma': ['scale', 0.001, 0.01, 0.1],\n",
    "    'model__kernel': ['rbf', 'poly', 'linear'],\n",
    "    'model__degree': [2, 3],\n",
    "    'model__coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__gamma': ['scale', 0.001, 0.01, 0.1],\n",
    "    'model__kernel': ['rbf', 'poly', 'linear'],\n",
    "    'model__degree': [2, 3],\n",
    "    'model__coef0': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        SVC(kernel='linear', random_state=RANDOM_STATE), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "svm_estimator = SVC(kernel='linear', random_state=RANDOM_STATE)\n",
    "rfecv = RFECV(\n",
    "    estimator=svm_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=svm_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.99\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', SVC(probability=True, random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run SVM with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING SVM WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                SVC(probability=True, random_state=RANDOM_STATE), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä Support Vector Machine Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Good generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'SVM',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('SVM')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SVM evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('svm_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce270e1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f560a",
   "metadata": {},
   "source": [
    "### Random Forest with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff8fe52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Delay', 'Visit', 'MR Delay']\n",
      "Categorical features: ['M/F']\n",
      "\n",
      "Preprocessed data shape: (647, 13)\n",
      "All features are now numeric: True\n",
      "Results cleared!\n",
      "\n",
      "=== START: Random Forest Configuration Sweep with Custom Hyperparameters ===\n",
      "\n",
      "‚úì Configuration 1: Preprocessed Data\n",
      "‚úì Configuration 2: Normalized Data (MinMax)\n",
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features: 12\n",
      "‚úì Configuration 3: SelectKBest\n",
      "\n",
      "=== RFECV Feature Selection ===\n",
      "Optimal number of features by RFECV: 2\n",
      "‚úì Configuration 4: RFECV\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components for 90.0% variance: 2\n",
      "‚úì Configuration 5: PCA\n",
      "‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\n",
      "‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\n",
      "\n",
      "=== Adding Undersampling Configurations ===\n",
      "‚úì Configuration 8: RandomUnderSampler (Undersampling)\n",
      "‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\n",
      "‚úì Configuration 10: NearMiss (Undersampling - selective)\n",
      "‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\n",
      "\n",
      "Total configurations: 11\n",
      "\n",
      "================================================================================\n",
      "RUNNING RANDOM FOREST WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Configuration: Preprocessed Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Preprocessed Data':\n",
      "  n_estimators: [50, 100, 200]\n",
      "  max_depth: [10, 20, 30, None]\n",
      "  min_samples_split: [2, 5, 10]\n",
      "  min_samples_leaf: [1, 2, 4]\n",
      "  max_features: ['sqrt', 'log2']\n",
      "  criterion: ['gini', 'entropy']\n",
      "  bootstrap: [True]\n",
      "Total combinations: 432\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.999932\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.990919\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  bootstrap: True\n",
      "  criterion: gini\n",
      "  max_depth: 10\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 10\n",
      "  n_estimators: 50\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "üå≤ Top 10 Most Important Features:\n",
      "  1. CDR: 0.3459\n",
      "  2. Educ: 0.2472\n",
      "  3. MMSE: 0.0859\n",
      "  4. MR Delay: 0.0800\n",
      "  5. nWBV: 0.0791\n",
      "  6. Visit: 0.0503\n",
      "  7. eTIV: 0.0316\n",
      "  8. Age: 0.0315\n",
      "  9. ASF: 0.0259\n",
      "  10. SES: 0.0123\n",
      "\n",
      "================================================================================\n",
      "Configuration: Normalized Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Normalized Data':\n",
      "  n_estimators: [100, 200, 300]\n",
      "  max_depth: [15, 25, 35, None]\n",
      "  min_samples_split: [2, 5]\n",
      "  min_samples_leaf: [1, 2]\n",
      "  max_features: ['sqrt', 'log2', None]\n",
      "  criterion: ['gini', 'entropy']\n",
      "  bootstrap: [True, False]\n",
      "Total combinations: 576\n",
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.998454  0.997783 0.996575   0.999004 1.000000\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.991784\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: 0.0108\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  bootstrap: True\n",
      "  criterion: entropy\n",
      "  max_depth: 15\n",
      "  max_features: None\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 200\n",
      "\n",
      "Best CV score: 0.960574\n",
      "\n",
      "üå≤ Top 10 Most Important Features:\n",
      "  1. Educ: 0.4361\n",
      "  2. CDR: 0.4023\n",
      "  3. Age: 0.0511\n",
      "  4. nWBV: 0.0284\n",
      "  5. MR Delay: 0.0243\n",
      "  6. eTIV: 0.0174\n",
      "  7. ASF: 0.0139\n",
      "  8. SES: 0.0116\n",
      "  9. MMSE: 0.0099\n",
      "  10. M/F_F: 0.0018\n",
      "\n",
      "================================================================================\n",
      "Configuration: SelectKBest\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SelectKBest':\n",
      "  n_estimators: [50, 100, 150, 200]\n",
      "  max_depth: [5, 10, 15, 20]\n",
      "  min_samples_split: [5, 10, 15]\n",
      "  min_samples_leaf: [2, 4, 6]\n",
      "  max_features: ['sqrt', 'log2']\n",
      "  criterion: ['gini']\n",
      "  bootstrap: [True]\n",
      "Total combinations: 288\n",
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.996172\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.990703\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  bootstrap: True\n",
      "  criterion: gini\n",
      "  max_depth: 5\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 10\n",
      "  n_estimators: 50\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "üå≤ Top 10 Most Important Features:\n",
      "  1. CDR: 0.3438\n",
      "  2. Educ: 0.2517\n",
      "  3. MMSE: 0.0989\n",
      "  4. MR Delay: 0.0965\n",
      "  5. nWBV: 0.0604\n",
      "  6. Age: 0.0580\n",
      "  7. Visit: 0.0463\n",
      "  8. eTIV: 0.0201\n",
      "  9. SES: 0.0116\n",
      "  10. M/F_F: 0.0070\n",
      "\n",
      "================================================================================\n",
      "Configuration: RFECV\n",
      "================================================================================\n",
      "Hyperparameter grid for 'RFECV':\n",
      "  n_estimators: [100, 200, 300]\n",
      "  max_depth: [10, 20, None]\n",
      "  min_samples_split: [2, 5, 10]\n",
      "  min_samples_leaf: [1, 2]\n",
      "  max_features: ['sqrt', 'log2', None]\n",
      "  criterion: ['gini', 'entropy']\n",
      "  bootstrap: [True]\n",
      "Total combinations: 324\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.982624\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.989622\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  bootstrap: True\n",
      "  criterion: gini\n",
      "  max_depth: 10\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  n_estimators: 100\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "üå≤ Top 10 Most Important Features:\n",
      "  1. CDR: 0.6463\n",
      "  2. Educ: 0.3537\n",
      "\n",
      "================================================================================\n",
      "Configuration: PCA\n",
      "================================================================================\n",
      "Hyperparameter grid for 'PCA':\n",
      "  n_estimators: [50, 100, 200, 300]\n",
      "  max_depth: [10, 15, 20, 25, None]\n",
      "  min_samples_split: [2, 5]\n",
      "  min_samples_leaf: [1, 2, 3]\n",
      "  max_features: ['sqrt', 'log2']\n",
      "  criterion: ['gini', 'entropy']\n",
      "  bootstrap: [True, False]\n",
      "Total combinations: 960\n",
      "Fitting 5 folds for each of 960 candidates, totalling 4800 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973725  0.960749 0.941781   0.983591 0.982624\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.990054\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0078\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  bootstrap: True\n",
      "  criterion: gini\n",
      "  max_depth: 10\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  n_estimators: 50\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "üå≤ Top 10 Most Important Features:\n",
      "  1. 1: 0.7328\n",
      "  2. 0: 0.2672\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + StandardScaler\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + StandardScaler':\n",
      "  model__n_estimators: [50, 100, 200]\n",
      "  model__max_depth: [10, 20, 30, None]\n",
      "  model__min_samples_split: [2, 5, 10]\n",
      "  model__min_samples_leaf: [1, 2, 4]\n",
      "  model__max_features: ['sqrt', 'log2']\n",
      "  model__criterion: ['gini', 'entropy']\n",
      "  model__bootstrap: [True]\n",
      "Total combinations: 432\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992272  0.988807 0.982877   0.995059 0.999959\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996108\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: 0.0046\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__bootstrap: True\n",
      "  model__criterion: gini\n",
      "  model__max_depth: 10\n",
      "  model__max_features: sqrt\n",
      "  model__min_samples_leaf: 1\n",
      "  model__min_samples_split: 10\n",
      "  model__n_estimators: 100\n",
      "\n",
      "Best CV score: 0.958264\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + GridSearchCV\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + GridSearchCV':\n",
      "  model__n_estimators: [50, 100, 200]\n",
      "  model__max_depth: [10, 20, 30, None]\n",
      "  model__min_samples_split: [2, 5, 10]\n",
      "  model__min_samples_leaf: [1, 2, 4]\n",
      "  model__max_features: ['sqrt', 'log2']\n",
      "  model__criterion: ['gini', 'entropy']\n",
      "  model__bootstrap: [True]\n",
      "Total combinations: 432\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.992272  0.988807 0.982877   0.995059 0.999959\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996108\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: 0.0046\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__bootstrap: True\n",
      "  model__criterion: gini\n",
      "  model__max_depth: 10\n",
      "  model__max_features: sqrt\n",
      "  model__min_samples_leaf: 1\n",
      "  model__min_samples_split: 10\n",
      "  model__n_estimators: 100\n",
      "\n",
      "Best CV score: 0.958264\n",
      "\n",
      "================================================================================\n",
      "Configuration: RandomUnderSampler\n",
      "================================================================================\n",
      "Hyperparameter grid for 'RandomUnderSampler':\n",
      "  model__n_estimators: [50, 100, 200]\n",
      "  model__max_depth: [10, 20, 30, None]\n",
      "  model__min_samples_split: [2, 5, 10]\n",
      "  model__min_samples_leaf: [1, 2, 4]\n",
      "  model__max_features: ['sqrt', 'log2']\n",
      "  model__criterion: ['gini', 'entropy']\n",
      "  model__bootstrap: [True]\n",
      "Total combinations: 432\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.982998  0.975616 0.974462   0.976783 0.999084\n",
      "    Test  0.981481  0.974447 0.988000   0.962500 0.992432\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: 0.0015\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__bootstrap: True\n",
      "  model__criterion: entropy\n",
      "  model__max_depth: 10\n",
      "  model__max_features: sqrt\n",
      "  model__min_samples_leaf: 1\n",
      "  model__min_samples_split: 10\n",
      "  model__n_estimators: 50\n",
      "\n",
      "Best CV score: 0.958334\n",
      "\n",
      "================================================================================\n",
      "Configuration: TomekLinks\n",
      "================================================================================\n",
      "Hyperparameter grid for 'TomekLinks':\n",
      "  model__n_estimators: [50, 100, 200]\n",
      "  model__max_depth: [10, 20, 30, None]\n",
      "  model__min_samples_split: [2, 5, 10]\n",
      "  model__min_samples_leaf: [1, 2, 4]\n",
      "  model__max_features: ['sqrt', 'log2']\n",
      "  model__criterion: ['gini', 'entropy']\n",
      "  model__bootstrap: [True]\n",
      "Total combinations: 432\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.976816  0.965550 0.948630   0.985465 0.999904\n",
      "    Test  0.981481  0.973976 0.978486   0.969652 0.995027\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0047\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__bootstrap: True\n",
      "  model__criterion: entropy\n",
      "  model__max_depth: 10\n",
      "  model__max_features: sqrt\n",
      "  model__min_samples_leaf: 1\n",
      "  model__min_samples_split: 10\n",
      "  model__n_estimators: 200\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "Configuration: NearMiss\n",
      "================================================================================\n",
      "Hyperparameter grid for 'NearMiss':\n",
      "  model__n_estimators: [50, 100, 200]\n",
      "  model__max_depth: [10, 20, 30, None]\n",
      "  model__min_samples_split: [2, 5, 10]\n",
      "  model__min_samples_leaf: [1, 2, 4]\n",
      "  model__max_features: ['sqrt', 'log2']\n",
      "  model__criterion: ['gini', 'entropy']\n",
      "  model__bootstrap: [True]\n",
      "Total combinations: 432\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996324\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: 0.0123\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__bootstrap: True\n",
      "  model__criterion: gini\n",
      "  model__max_depth: 10\n",
      "  model__max_features: sqrt\n",
      "  model__min_samples_leaf: 1\n",
      "  model__min_samples_split: 2\n",
      "  model__n_estimators: 100\n",
      "\n",
      "Best CV score: 0.958389\n",
      "\n",
      "================================================================================\n",
      "Configuration: SMOTE + Tomek\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SMOTE + Tomek':\n",
      "  model__n_estimators: [50, 100, 200]\n",
      "  model__max_depth: [10, 20, 30, None]\n",
      "  model__min_samples_split: [2, 5, 10]\n",
      "  model__min_samples_leaf: [1, 2, 4]\n",
      "  model__max_features: ['sqrt', 'log2']\n",
      "  model__criterion: ['gini', 'entropy']\n",
      "  model__bootstrap: [True]\n",
      "Total combinations: 432\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "\n",
      "üìä Random Forest Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.982998  0.974998 0.962329   0.989258 0.999904\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.995243\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: -0.0047\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  model__bootstrap: True\n",
      "  model__criterion: gini\n",
      "  model__max_depth: 10\n",
      "  model__max_features: sqrt\n",
      "  model__min_samples_leaf: 4\n",
      "  model__min_samples_split: 10\n",
      "  model__n_estimators: 100\n",
      "\n",
      "Best CV score: 0.960400\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Random Forest evaluation complete for all configurations.\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "MODEL PERFORMANCE RESULTS\n",
      "====================================================================================================\n",
      "     ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "Random Forest      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.990919\n",
      "Random Forest        Normalized Data  0.987654  0.982810 0.992000   0.974359 0.991784\n",
      "Random Forest            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.990703\n",
      "Random Forest                  RFECV  0.981481  0.973976 0.978486   0.969652 0.989622\n",
      "Random Forest                    PCA  0.981481  0.973976 0.978486   0.969652 0.990054\n",
      "Random Forest SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996108\n",
      "Random Forest   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996108\n",
      "Random Forest     RandomUnderSampler  0.981481  0.974447 0.988000   0.962500 0.992432\n",
      "Random Forest             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.995027\n",
      "Random Forest               NearMiss  0.987654  0.982810 0.992000   0.974359 0.996324\n",
      "Random Forest          SMOTE + Tomek  0.987654  0.982810 0.992000   0.974359 0.995243\n",
      "\n",
      "üìà Final Results:\n",
      "     ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "Random Forest      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.990919\n",
      "Random Forest        Normalized Data  0.987654  0.982810 0.992000   0.974359 0.991784\n",
      "Random Forest            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.990703\n",
      "Random Forest                  RFECV  0.981481  0.973976 0.978486   0.969652 0.989622\n",
      "Random Forest                    PCA  0.981481  0.973976 0.978486   0.969652 0.990054\n",
      "Random Forest SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996108\n",
      "Random Forest   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996108\n",
      "Random Forest     RandomUnderSampler  0.981481  0.974447 0.988000   0.962500 0.992432\n",
      "Random Forest             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.995027\n",
      "Random Forest               NearMiss  0.987654  0.982810 0.992000   0.974359 0.996324\n",
      "Random Forest          SMOTE + Tomek  0.987654  0.982810 0.992000   0.974359 0.995243\n",
      "\n",
      "üèÜ Sorted Results (by F1 Score):\n",
      "     ML Model          Configuration  Accuracy  F1 Score   Recall  Precision  ROC_AUC\n",
      "Random Forest        Normalized Data  0.987654  0.982810 0.992000   0.974359 0.991784\n",
      "Random Forest SMOTE + StandardScaler  0.987654  0.982810 0.992000   0.974359 0.996108\n",
      "Random Forest   SMOTE + GridSearchCV  0.987654  0.982810 0.992000   0.974359 0.996108\n",
      "Random Forest               NearMiss  0.987654  0.982810 0.992000   0.974359 0.996324\n",
      "Random Forest          SMOTE + Tomek  0.987654  0.982810 0.992000   0.974359 0.995243\n",
      "Random Forest     RandomUnderSampler  0.981481  0.974447 0.988000   0.962500 0.992432\n",
      "Random Forest      Preprocessed Data  0.981481  0.973976 0.978486   0.969652 0.990919\n",
      "Random Forest            SelectKBest  0.981481  0.973976 0.978486   0.969652 0.990703\n",
      "Random Forest                  RFECV  0.981481  0.973976 0.978486   0.969652 0.989622\n",
      "Random Forest                    PCA  0.981481  0.973976 0.978486   0.969652 0.990054\n",
      "Random Forest             TomekLinks  0.981481  0.973976 0.978486   0.969652 0.995027\n",
      "\n",
      "üìä Performance by Sampling Technique:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "No Sampling:\n",
      "  Avg Accuracy: 0.982716\n",
      "  Avg F1 Score: 0.975743\n",
      "  Avg ROC-AUC: 0.990616\n",
      "  Best Config: Normalized Data\n",
      "\n",
      "Oversampling:\n",
      "  Avg Accuracy: 0.987654\n",
      "  Avg F1 Score: 0.982810\n",
      "  Avg ROC-AUC: 0.996108\n",
      "  Best Config: SMOTE + StandardScaler\n",
      "\n",
      "Undersampling:\n",
      "  Avg Accuracy: 0.983539\n",
      "  Avg F1 Score: 0.977078\n",
      "  Avg ROC-AUC: 0.994594\n",
      "  Best Config: NearMiss\n",
      "\n",
      "Combined:\n",
      "  Avg Accuracy: 0.987654\n",
      "  Avg F1 Score: 0.982810\n",
      "  Avg ROC-AUC: 0.995243\n",
      "  Best Config: SMOTE + Tomek\n",
      "\n",
      "üèÖ Best Overall Configuration:\n",
      "  Configuration: Normalized Data\n",
      "  F1 Score: 0.982810\n",
      "  Accuracy: 0.987654\n",
      "  ROC-AUC: 0.991784\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Random Forest with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: Random Forest Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for Random Forest\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Balanced exploration\n",
    "param_grid_1 = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on deeper trees\n",
    "param_grid_2 = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [15, 25, 35, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative (prevent overfitting)\n",
    "param_grid_3 = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "    'min_samples_leaf': [2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini'],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Focus on feature importance\n",
    "param_grid_4 = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Simpler models for reduced dimensions\n",
    "param_grid_5 = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, 25, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [10, 20, 30, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt', 'log2'],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [10, 20, 30, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt', 'log2'],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "rf_estimator = RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE)\n",
    "rfecv = RFECV(\n",
    "    estimator=rf_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=rf_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run Random Forest with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING RANDOM FOREST WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                RandomForestClassifier(random_state=RANDOM_STATE), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä Random Forest Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Good generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'Random Forest',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('Random Forest')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Feature importance for non-pipeline configurations\n",
    "        if kind == 'array' and hasattr(best_model, 'feature_importances_'):\n",
    "            print(\"\\nüå≤ Top 10 Most Important Features:\")\n",
    "            importances = best_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]\n",
    "            feature_cols = X_tr_cfg.columns if hasattr(X_tr_cfg, 'columns') else [f'Feature {i}' for i in range(len(importances))]\n",
    "            for i, idx in enumerate(indices, 1):\n",
    "                print(f\"  {i}. {feature_cols[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Random Forest evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('random_forest_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16942ac3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0f7b8",
   "metadata": {},
   "source": [
    "### Gradient Boosting with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe6d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Delay', 'Visit', 'MR Delay']\n",
      "Categorical features: ['M/F']\n",
      "\n",
      "Preprocessed data shape: (647, 13)\n",
      "All features are now numeric: True\n",
      "Initialized result storage lists.\n",
      "\n",
      "=== START: Gradient Boosting Configuration Sweep with Custom Hyperparameters ===\n",
      "\n",
      "‚úì Configuration 1: Preprocessed Data\n",
      "‚úì Configuration 2: Normalized Data (MinMax)\n",
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features: 6\n",
      "‚úì Configuration 3: SelectKBest\n",
      "\n",
      "=== RFECV Feature Selection ===\n",
      "Optimal number of features by RFECV: 2\n",
      "‚úì Configuration 4: RFECV\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components for 90.0% variance: 2\n",
      "‚úì Configuration 5: PCA\n",
      "‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\n",
      "‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\n",
      "\n",
      "=== Adding Undersampling Configurations ===\n",
      "‚úì Configuration 8: RandomUnderSampler (Undersampling)\n",
      "‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\n",
      "‚úì Configuration 10: NearMiss (Undersampling - selective)\n",
      "‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\n",
      "\n",
      "Total configurations: 11\n",
      "\n",
      "================================================================================\n",
      "RUNNING GRADIENT BOOSTING WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Configuration: Preprocessed Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Preprocessed Data':\n",
      "  learning_rate: [0.01, 0.05, 0.1, 0.2]\n",
      "  n_estimators: [100, 200, 300]\n",
      "  max_depth: [3, 4, 5]\n",
      "  min_samples_split: [2, 5, 10]\n",
      "  min_samples_leaf: [1, 2, 4]\n",
      "  subsample: [0.8, 1.0]\n",
      "  max_features: ['sqrt', 'log2']\n",
      "Total combinations: 1296\n",
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "\n",
      "üìä Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.998054\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: 0.0123\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  learning_rate: 0.2\n",
      "  max_depth: 5\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 100\n",
      "  subsample: 0.8\n",
      "\n",
      "Best CV score: 0.963074\n",
      "\n",
      "üéØ Top 10 Most Important Features:\n",
      "  1. CDR: 0.3417\n",
      "  2. Educ: 0.2480\n",
      "  3. Age: 0.0806\n",
      "  4. Visit: 0.0805\n",
      "  5. nWBV: 0.0706\n",
      "  6. MR Delay: 0.0591\n",
      "  7. MMSE: 0.0447\n",
      "  8. ASF: 0.0293\n",
      "  9. eTIV: 0.0275\n",
      "  10. SES: 0.0123\n",
      "\n",
      "================================================================================\n",
      "Configuration: Normalized Data\n",
      "================================================================================\n",
      "Hyperparameter grid for 'Normalized Data':\n",
      "  learning_rate: [0.01, 0.05, 0.1]\n",
      "  n_estimators: [100, 200, 300, 500]\n",
      "  max_depth: [3, 4, 5, 6]\n",
      "  min_samples_split: [2, 5]\n",
      "  min_samples_leaf: [1, 2]\n",
      "  subsample: [0.6, 0.8, 1.0]\n",
      "  max_features: ['sqrt', 'log2', None]\n",
      "Total combinations: 1728\n",
      "Fitting 5 folds for each of 1728 candidates, totalling 8640 fits\n",
      "\n",
      "üìä Gradient Boosting Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.998454  0.997783 0.996575   0.999004 1.000000\n",
      "    Test  0.987654  0.982810 0.992000   0.974359 0.996541\n",
      "\n",
      "‚úì Good generalization. Train-Test gap: 0.0108\n",
      "\n",
      "üéØ Best hyperparameters found:\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 3\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 300\n",
      "  subsample: 0.8\n",
      "\n",
      "Best CV score: 0.961018\n",
      "\n",
      "üéØ Top 10 Most Important Features:\n",
      "  1. CDR: 0.3410\n",
      "  2. Educ: 0.2403\n",
      "  3. MMSE: 0.1211\n",
      "  4. nWBV: 0.0658\n",
      "  5. Visit: 0.0618\n",
      "  6. MR Delay: 0.0581\n",
      "  7. Age: 0.0548\n",
      "  8. eTIV: 0.0234\n",
      "  9. ASF: 0.0208\n",
      "  10. SES: 0.0069\n",
      "\n",
      "================================================================================\n",
      "Configuration: SelectKBest\n",
      "================================================================================\n",
      "Hyperparameter grid for 'SelectKBest':\n",
      "  learning_rate: [0.01, 0.05, 0.1, 0.15]\n",
      "  n_estimators: [50, 100, 200, 300]\n",
      "  max_depth: [2, 3, 4, 5]\n",
      "  min_samples_split: [5, 10, 20]\n",
      "  min_samples_leaf: [2, 4, 6]\n",
      "  subsample: [0.7, 0.8, 0.9]\n",
      "  max_features: ['sqrt', 'log2']\n",
      "Total combinations: 3456\n",
      "Fitting 5 folds for each of 3456 candidates, totalling 17280 fits\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Gradient Boosting with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: Gradient Boosting Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for Gradient Boosting\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Balanced exploration\n",
    "param_grid_1 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on learning rate and subsample\n",
    "param_grid_2 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative (prevent overfitting)\n",
    "param_grid_3 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'min_samples_leaf': [2, 4, 6],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Focus on sequential boosting\n",
    "param_grid_4 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Simpler models for reduced dimensions\n",
    "param_grid_5 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=RANDOM_STATE), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "gb_estimator = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=RANDOM_STATE)\n",
    "rfecv = RFECV(\n",
    "    estimator=gb_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=gb_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run Gradient Boosting with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING GRADIENT BOOSTING WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                GradientBoostingClassifier(random_state=RANDOM_STATE), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä Gradient Boosting Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Good generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'Gradient Boosting',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('Gradient Boosting')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Feature importance for non-pipeline configurations\n",
    "        if kind == 'array' and hasattr(best_model, 'feature_importances_'):\n",
    "            print(\"\\nüéØ Top 10 Most Important Features:\")\n",
    "            importances = best_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]\n",
    "            feature_cols = X_tr_cfg.columns if hasattr(X_tr_cfg, 'columns') else [f'Feature {i}' for i in range(len(importances))]\n",
    "            for i, idx in enumerate(indices, 1):\n",
    "                print(f\"  {i}. {feature_cols[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Gradient Boosting evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('gradient_boosting_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5471f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccff270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Decision Tree with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: Decision Tree Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for Decision Tree\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Balanced exploration with pruning\n",
    "param_grid_1 = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [3, 5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_impurity_decrease': [0.0, 0.0001, 0.001],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on depth and splitting criteria\n",
    "param_grid_2 = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best'],\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_impurity_decrease': [0.0, 0.0001, 0.0005, 0.001],\n",
    "    'ccp_alpha': [0.0]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative (prevent overfitting)\n",
    "param_grid_3 = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best'],\n",
    "    'max_depth': [3, 5, 8, 10, 15],\n",
    "    'min_samples_split': [5, 10, 15, 20],\n",
    "    'min_samples_leaf': [2, 4, 6, 8, 10],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_impurity_decrease': [0.0001, 0.001, 0.005],\n",
    "    'ccp_alpha': [0.0, 0.005, 0.01, 0.02]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Explore complexity with pruning\n",
    "param_grid_4 = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_impurity_decrease': [0.0, 0.0005, 0.001, 0.005],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.005]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Simpler trees for reduced dimensions\n",
    "param_grid_5 = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best'],\n",
    "    'max_depth': [5, 10, 15, 20, 25, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_impurity_decrease': [0.0, 0.0001, 0.001, 0.01],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__splitter': ['best'],\n",
    "    'model__max_depth': [5, 10, 15, 20, None],\n",
    "    'model__min_samples_split': [2, 5, 10, 20],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 8],\n",
    "    'model__max_features': ['sqrt', 'log2'],\n",
    "    'model__min_impurity_decrease': [0.0, 0.0001, 0.001],\n",
    "    'model__ccp_alpha': [0.0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__splitter': ['best'],\n",
    "    'model__max_depth': [5, 10, 15, 20, None],\n",
    "    'model__min_samples_split': [2, 5, 10, 20],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 8],\n",
    "    'model__max_features': ['sqrt', 'log2'],\n",
    "    'model__min_impurity_decrease': [0.0, 0.0001, 0.001],\n",
    "    'model__ccp_alpha': [0.0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        DecisionTreeClassifier(random_state=RANDOM_STATE), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "dt_estimator = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "rfecv = RFECV(\n",
    "    estimator=dt_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=dt_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run Decision Tree with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING DECISION TREE WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                DecisionTreeClassifier(random_state=RANDOM_STATE), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä Decision Tree Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning (Decision Trees are prone to overfitting)\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "            print(\"   Consider: increasing min_samples_split, min_samples_leaf, or ccp_alpha\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Excellent generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'Decision Tree',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('Decision Tree')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Tree complexity metrics\n",
    "        if kind == 'array':\n",
    "            tree_model = best_model\n",
    "        else:\n",
    "            tree_model = best_model.named_steps['model']\n",
    "        \n",
    "        print(f\"\\nüå≥ Tree Complexity Metrics:\")\n",
    "        print(f\"  Number of leaves: {tree_model.get_n_leaves()}\")\n",
    "        print(f\"  Tree depth: {tree_model.get_depth()}\")\n",
    "        \n",
    "        # Feature importance for non-pipeline configurations\n",
    "        if kind == 'array' and hasattr(best_model, 'feature_importances_'):\n",
    "            print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "            importances = best_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]\n",
    "            feature_cols = X_tr_cfg.columns if hasattr(X_tr_cfg, 'columns') else [f'Feature {i}' for i in range(len(importances))]\n",
    "            for i, idx in enumerate(indices, 1):\n",
    "                print(f\"  {i}. {feature_cols[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Decision Tree evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('decision_tree_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643e704",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11db531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LightGBM with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: LightGBM Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for LightGBM\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Balanced exploration\n",
    "param_grid_1 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1],\n",
    "    'reg_lambda': [0.0, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on tree complexity\n",
    "param_grid_2 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'num_leaves': [31, 50, 70, 100],\n",
    "    'max_depth': [-1, 7, 10],\n",
    "    'min_child_samples': [10, 20, 40],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative (prevent overfitting)\n",
    "param_grid_3 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'num_leaves': [20, 31, 50],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_samples': [20, 40, 60],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'reg_alpha': [0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Focus on feature importance\n",
    "param_grid_4 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'max_depth': [-1, 5, 10, 15],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Optimized for reduced dimensions\n",
    "param_grid_5 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'num_leaves': [31, 50, 70, 100],\n",
    "    'max_depth': [-1, 7, 10],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1],\n",
    "    'reg_lambda': [0.0, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__num_leaves': [31, 50, 70],\n",
    "    'model__max_depth': [-1, 5, 10],\n",
    "    'model__min_child_samples': [10, 20, 30],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8],\n",
    "    'model__reg_alpha': [0.0, 0.1],\n",
    "    'model__reg_lambda': [0.0, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__num_leaves': [31, 50, 70],\n",
    "    'model__max_depth': [-1, 5, 10],\n",
    "    'model__min_child_samples': [10, 20, 30],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8],\n",
    "    'model__reg_alpha': [0.0, 0.1],\n",
    "    'model__reg_lambda': [0.0, 0.1]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        LGBMClassifier(n_estimators=50, learning_rate=0.1, random_state=RANDOM_STATE, verbose=-1), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "lgbm_estimator = LGBMClassifier(n_estimators=50, learning_rate=0.1, random_state=RANDOM_STATE, verbose=-1)\n",
    "rfecv = RFECV(\n",
    "    estimator=lgbm_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=lgbm_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', LGBMClassifier(random_state=RANDOM_STATE, verbose=-1))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', LGBMClassifier(random_state=RANDOM_STATE, verbose=-1))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', LGBMClassifier(random_state=RANDOM_STATE, verbose=-1))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', LGBMClassifier(random_state=RANDOM_STATE, verbose=-1))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', LGBMClassifier(random_state=RANDOM_STATE, verbose=-1))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', LGBMClassifier(random_state=RANDOM_STATE, verbose=-1))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run LightGBM with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING LIGHTGBM WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                LGBMClassifier(random_state=RANDOM_STATE, verbose=-1), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä LightGBM Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "            print(\"   Consider: increasing min_child_samples, reg_alpha/reg_lambda, or reducing num_leaves\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Excellent generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'LightGBM',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('LightGBM')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Feature importance for non-pipeline configurations\n",
    "        if kind == 'array' and hasattr(best_model, 'feature_importances_'):\n",
    "            print(\"\\nüí° Top 10 Most Important Features:\")\n",
    "            importances = best_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]\n",
    "            feature_cols = X_tr_cfg.columns if hasattr(X_tr_cfg, 'columns') else [f'Feature {i}' for i in range(len(importances))]\n",
    "            for i, idx in enumerate(indices, 1):\n",
    "                print(f\"  {i}. {feature_cols[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ LightGBM evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('lightgbm_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ea7fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CatBoost with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: CatBoost Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for CatBoost\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Balanced exploration\n",
    "param_grid_1 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'iterations': [100, 200, 300],\n",
    "    'depth': [4, 6, 8],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'bagging_temperature': [0, 0.5, 1],\n",
    "    'random_strength': [0, 1, 5],\n",
    "    'border_count': [32, 128, 254],\n",
    "    'subsample': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on tree complexity\n",
    "param_grid_2 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'iterations': [100, 200, 300, 500],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 10],\n",
    "    'bagging_temperature': [0, 0.5, 1],\n",
    "    'random_strength': [0, 1, 5, 10],\n",
    "    'border_count': [32, 128],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative (prevent overfitting)\n",
    "param_grid_3 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'iterations': [100, 200, 300],\n",
    "    'depth': [4, 5, 6],\n",
    "    'l2_leaf_reg': [3, 5, 10, 20],\n",
    "    'bagging_temperature': [0, 0.5],\n",
    "    'random_strength': [1, 5, 10],\n",
    "    'border_count': [32, 64],\n",
    "    'subsample': [0.7, 0.8]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Focus on feature importance\n",
    "param_grid_4 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'iterations': [100, 200, 300, 400],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'bagging_temperature': [0, 0.5, 1],\n",
    "    'random_strength': [0, 1, 5],\n",
    "    'border_count': [32, 128],\n",
    "    'subsample': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Optimized for reduced dimensions\n",
    "param_grid_5 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'iterations': [100, 200, 300, 500],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'bagging_temperature': [0.5, 1],\n",
    "    'random_strength': [0, 1],\n",
    "    'border_count': [128, 254],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__iterations': [100, 200, 300],\n",
    "    'model__depth': [4, 6, 8],\n",
    "    'model__l2_leaf_reg': [1, 3, 5],\n",
    "    'model__bagging_temperature': [0, 0.5, 1],\n",
    "    'model__random_strength': [0, 1, 5],\n",
    "    'model__border_count': [32, 128],\n",
    "    'model__subsample': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__iterations': [100, 200, 300],\n",
    "    'model__depth': [4, 6, 8],\n",
    "    'model__l2_leaf_reg': [1, 3, 5],\n",
    "    'model__bagging_temperature': [0, 0.5, 1],\n",
    "    'model__random_strength': [0, 1, 5],\n",
    "    'model__border_count': [32, 128],\n",
    "    'model__subsample': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=RANDOM_STATE, verbose=0), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "cb_estimator = CatBoostClassifier(iterations=50, learning_rate=0.1, random_state=RANDOM_STATE, verbose=0)\n",
    "rfecv = RFECV(\n",
    "    estimator=cb_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=cb_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', CatBoostClassifier(random_state=RANDOM_STATE, verbose=0))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', CatBoostClassifier(random_state=RANDOM_STATE, verbose=0))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', CatBoostClassifier(random_state=RANDOM_STATE, verbose=0))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', CatBoostClassifier(random_state=RANDOM_STATE, verbose=0))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', CatBoostClassifier(random_state=RANDOM_STATE, verbose=0))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', CatBoostClassifier(random_state=RANDOM_STATE, verbose=0))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run CatBoost with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING CATBOOST WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                CatBoostClassifier(random_state=RANDOM_STATE, verbose=0), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä CatBoost Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "            print(\"   Consider: increasing l2_leaf_reg, reducing depth, or adjusting bagging_temperature\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Excellent generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'CatBoost',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('CatBoost')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Feature importance for non-pipeline configurations\n",
    "        if kind == 'array' and hasattr(best_model, 'feature_importances_'):\n",
    "            print(\"\\nüèÜ Top 10 Most Important Features:\")\n",
    "            importances = best_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]\n",
    "            feature_cols = X_tr_cfg.columns if hasattr(X_tr_cfg, 'columns') else [f'Feature {i}' for i in range(len(importances))]\n",
    "            for i, idx in enumerate(indices, 1):\n",
    "                print(f\"  {i}. {feature_cols[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CatBoost evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('catboost_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f7fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8f838",
   "metadata": {},
   "source": [
    "### Adaboost with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4da461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 25\n",
      "\n",
      "=== RFECV Feature Selection with AdaBoost ===\n",
      "Optimal number of features selected by RFECV: 11\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 10\n",
      "\n",
      "=== AdaBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running AdaBoost with Original Data configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.966825  0.860668 0.805082   0.942229 0.992096\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.966825  0.860668 0.805082   0.942229 0.992096\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000  1.00000\n",
      "    Test  0.962085  0.818235 0.723485   0.986667  0.98906\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with RFECV configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.984177  0.943348 0.935185   0.952095 0.998158\n",
      "    Test  0.947867  0.747080 0.661143   0.916750 0.967564\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 0.1, 'n_estimators': 200}\n",
      "\n",
      "Running AdaBoost with PCA configuration...\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "\n",
      "AdaBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.938389  0.693736 0.629104   0.842869 0.961022\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'algorithm': 'SAMME', 'estimator': DecisionTreeClassifier(max_depth=5), 'learning_rate': 1, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AdaBoost with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: AdaBoost Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for AdaBoost\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Balanced exploration\n",
    "param_grid_1 = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'estimator__max_depth': [1, 2, 3],\n",
    "    'estimator__min_samples_split': [2, 5],\n",
    "    'estimator__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on more estimators\n",
    "param_grid_2 = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5],\n",
    "    'algorithm': ['SAMME.R'],\n",
    "    'estimator__max_depth': [1, 2, 3, 4],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative (prevent overfitting)\n",
    "param_grid_3 = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'algorithm': ['SAMME.R'],\n",
    "    'estimator__max_depth': [1, 2],\n",
    "    'estimator__min_samples_split': [5, 10, 20],\n",
    "    'estimator__min_samples_leaf': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Explore algorithm variations\n",
    "param_grid_4 = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'estimator__max_depth': [1, 2, 3],\n",
    "    'estimator__min_samples_split': [2, 5],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Simpler weak learners\n",
    "param_grid_5 = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME.R'],\n",
    "    'estimator__max_depth': [1, 2, 3],\n",
    "    'estimator__min_samples_split': [2, 5],\n",
    "    'estimator__min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'model__algorithm': ['SAMME.R'],\n",
    "    'model__estimator__max_depth': [1, 2, 3],\n",
    "    'model__estimator__min_samples_split': [2, 5],\n",
    "    'model__estimator__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'model__algorithm': ['SAMME.R'],\n",
    "    'model__estimator__max_depth': [1, 2, 3],\n",
    "    'model__estimator__min_samples_split': [2, 5],\n",
    "    'model__estimator__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=RANDOM_STATE), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "ada_estimator = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=RANDOM_STATE)\n",
    "rfecv = RFECV(\n",
    "    estimator=ada_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=ada_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run AdaBoost with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING ADABOOST WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä AdaBoost Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "            print(\"   Consider: reducing n_estimators, lowering learning_rate, or increasing min_samples_split\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Excellent generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'AdaBoost',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('AdaBoost')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Display number of estimators used\n",
    "        print(f\"\\nüìà Model Complexity:\")\n",
    "        print(f\"  Number of estimators: {best_model.n_estimators}\")\n",
    "        if hasattr(best_model, 'estimators_'):\n",
    "            print(f\"  Actual estimators used: {len(best_model.estimators_)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AdaBoost evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('adaboost_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911a9a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76533d1",
   "metadata": {},
   "source": [
    "### XGBoost with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 15\n",
      "\n",
      "=== RFECV Feature Selection with XGBoost ===\n",
      "Optimal number of features selected by RFECV: 15\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 13\n",
      "\n",
      "=== XGBoost Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running XGBoost with Original Data configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.952607  0.755429 0.651515   0.983498 0.986044\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
      "    Test  0.952607  0.755429 0.651515   0.983498 0.986044\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.952607  0.753445 0.674242   0.983498 0.956643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with RFECV configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.996835  0.987039 0.975694   0.998847 0.999959\n",
      "    Test  0.952607  0.753445 0.674242   0.983498 0.956643\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "Running XGBoost with PCA configuration...\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "\n",
      "XGBoost Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.973101  0.883080 0.818866   0.975103 0.991660\n",
      "    Test  0.943128  0.682207 0.602273   0.980392 0.949287\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# XGBoost with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: XGBoost Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for XGBoost\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Balanced exploration\n",
    "param_grid_1 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'reg_lambda': [1, 1.5]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on tree complexity\n",
    "param_grid_2 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative (prevent overfitting)\n",
    "param_grid_3 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'gamma': [0.1, 0.5, 1.0],\n",
    "    'reg_alpha': [0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Focus on feature importance\n",
    "param_grid_4 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Optimized for reduced dimensions\n",
    "param_grid_5 = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 0.1],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'reg_lambda': [1, 1.5]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data (Oversampling)\n",
    "param_grid_smote = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__min_child_weight': [1, 3, 5],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8],\n",
    "    'model__gamma': [0, 0.1, 0.5],\n",
    "    'model__reg_alpha': [0, 0.1],\n",
    "    'model__reg_lambda': [1, 1.5]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__min_child_weight': [1, 3, 5],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8],\n",
    "    'model__gamma': [0, 0.1, 0.5],\n",
    "    'model__reg_alpha': [0, 0.1],\n",
    "    'model__reg_lambda': [1, 1.5]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    score = cross_val_score(\n",
    "        XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=RANDOM_STATE, eval_metric='logloss'), \n",
    "        X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "xgb_estimator = XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "rfecv = RFECV(\n",
    "    estimator=xgb_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=xgb_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run XGBoost with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING XGBOOST WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            grid_search = GridSearchCV(\n",
    "                XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'), \n",
    "                param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä XGBoost Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "            print(\"   Consider: reducing max_depth, increasing min_child_weight, or increasing gamma/regularization\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Excellent generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'XGBoost',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('XGBoost')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Feature importance for non-pipeline configurations\n",
    "        if kind == 'array' and hasattr(best_model, 'feature_importances_'):\n",
    "            print(\"\\nüöÄ Top 10 Most Important Features:\")\n",
    "            importances = best_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]\n",
    "            feature_cols = X_tr_cfg.columns if hasattr(X_tr_cfg, 'columns') else [f'Feature {i}' for i in range(len(importances))]\n",
    "            for i, idx in enumerate(indices, 1):\n",
    "                print(f\"  {i}. {feature_cols[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ XGBoost evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('xgboost_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccbad96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2517b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Voting Classifier with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: Voting Classifier Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for Voting Classifier\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Explore voting types and weights\n",
    "param_grid_1 = {\n",
    "    'voting': ['soft', 'hard'],\n",
    "    'weights': [(1,1,1,1), (2,1,1,1), (1,2,1,1), (1,1,2,1), (1,1,1,2)],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [10, 20],\n",
    "    'gb__n_estimators': [100, 200],\n",
    "    'gb__learning_rate': [0.05, 0.1],\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on individual estimator tuning\n",
    "param_grid_2 = {\n",
    "    'voting': ['soft'],\n",
    "    'weights': [(1,1,1,1), (2,1,1,1), (1,1,2,1)],\n",
    "    'lr__C': [0.1, 1.0, 10],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [15, 25],\n",
    "    'gb__n_estimators': [100, 200],\n",
    "    'gb__learning_rate': [0.01, 0.1],\n",
    "    'xgb__learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative parameters\n",
    "param_grid_3 = {\n",
    "    'voting': ['soft', 'hard'],\n",
    "    'weights': [(1,1,1,1), (2,1,1,1)],\n",
    "    'rf__n_estimators': [50, 100],\n",
    "    'rf__max_depth': [5, 10],\n",
    "    'rf__min_samples_split': [5, 10],\n",
    "    'gb__n_estimators': [50, 100],\n",
    "    'gb__learning_rate': [0.05, 0.1],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__reg_alpha': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Balanced tuning\n",
    "param_grid_4 = {\n",
    "    'voting': ['soft'],\n",
    "    'weights': [(1,1,1,1), (1,2,1,1), (1,1,1,2)],\n",
    "    'lr__C': [1.0, 10],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [10, 20],\n",
    "    'gb__learning_rate': [0.05, 0.1],\n",
    "    'xgb__n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Optimized for reduced dimensions\n",
    "param_grid_5 = {\n",
    "    'voting': ['soft', 'hard'],\n",
    "    'weights': [(1,1,1,1), (2,1,1,1), (1,1,2,1)],\n",
    "    'lr__C': [0.1, 1.0],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'gb__n_estimators': [100, 200],\n",
    "    'gb__learning_rate': [0.1, 0.2],\n",
    "    'xgb__learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data\n",
    "param_grid_smote = {\n",
    "    'model__voting': ['soft'],\n",
    "    'model__weights': [(1,1,1,1), (2,1,1,1)],\n",
    "    'model__rf__n_estimators': [100, 200],\n",
    "    'model__rf__max_depth': [10, 20],\n",
    "    'model__gb__n_estimators': [100, 200],\n",
    "    'model__xgb__max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__voting': ['soft'],\n",
    "    'model__weights': [(1,1,1,1), (2,1,1,1)],\n",
    "    'model__rf__n_estimators': [100, 200],\n",
    "    'model__gb__learning_rate': [0.05, 0.1],\n",
    "    'model__xgb__n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "# Create base estimators for voting\n",
    "lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "gb = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "xgb = XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    \n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('lr', lr), ('rf', rf), ('gb', gb), ('xgb', xgb)],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(\n",
    "        voting_clf, X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "voting_estimator = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('rf', rf), ('gb', gb), ('xgb', xgb)],\n",
    "    voting='soft'\n",
    ")\n",
    "rfecv = RFECV(\n",
    "    estimator=voting_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=voting_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run Voting Classifier with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING VOTING CLASSIFIER WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            # Create voting classifier with base estimators\n",
    "            voting_clf = VotingClassifier(\n",
    "                estimators=[\n",
    "                    ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "                    ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "                    ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "                    ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "                ],\n",
    "                voting='soft'\n",
    "            )\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                voting_clf, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä Voting Classifier Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "            print(\"   Consider: adjusting individual estimator parameters or changing voting weights\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Excellent generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'Voting Classifier',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('Voting Classifier')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Display estimator information\n",
    "        print(f\"\\nüó≥Ô∏è Voting Ensemble Details:\")\n",
    "        print(f\"  Voting Type: {best_model.voting}\")\n",
    "        if hasattr(best_model, 'weights') and best_model.weights is not None:\n",
    "            print(f\"  Estimator Weights: {best_model.weights}\")\n",
    "        print(f\"  Number of Base Estimators: {len(best_model.estimators_)}\")\n",
    "        print(f\"  Base Estimators: {[name for name, _ in best_model.estimators]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Voting Classifier evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('voting_classifier_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba61ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stacking Classifier with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: Stacking Classifier Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for Stacking Classifier\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Explore meta-learners and passthrough\n",
    "param_grid_1 = {\n",
    "    'passthrough': [False, True],\n",
    "    'cv': [5],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [10, 20],\n",
    "    'gb__n_estimators': [100, 200],\n",
    "    'gb__learning_rate': [0.05, 0.1],\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'final_estimator__C': [0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "# Grid 2: Normalized Data - Focus on meta-learner tuning\n",
    "param_grid_2 = {\n",
    "    'passthrough': [False, True],\n",
    "    'cv': [5],\n",
    "    'lr__C': [0.1, 1.0, 10],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [15, 25],\n",
    "    'gb__n_estimators': [100, 200],\n",
    "    'gb__learning_rate': [0.01, 0.1],\n",
    "    'xgb__learning_rate': [0.05, 0.1],\n",
    "    'final_estimator__C': [1.0, 10]\n",
    "}\n",
    "\n",
    "# Grid 3: SelectKBest - More conservative parameters\n",
    "param_grid_3 = {\n",
    "    'passthrough': [False],\n",
    "    'cv': [5],\n",
    "    'rf__n_estimators': [50, 100],\n",
    "    'rf__max_depth': [5, 10],\n",
    "    'rf__min_samples_split': [5, 10],\n",
    "    'gb__n_estimators': [50, 100],\n",
    "    'gb__learning_rate': [0.05, 0.1],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__reg_alpha': [0, 0.1],\n",
    "    'final_estimator__C': [1.0, 10]\n",
    "}\n",
    "\n",
    "# Grid 4: RFECV - Balanced tuning with passthrough\n",
    "param_grid_4 = {\n",
    "    'passthrough': [False, True],\n",
    "    'cv': [5],\n",
    "    'lr__C': [1.0, 10],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [10, 20],\n",
    "    'gb__learning_rate': [0.05, 0.1],\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'final_estimator__C': [0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Grid 5: PCA - Optimized for reduced dimensions\n",
    "param_grid_5 = {\n",
    "    'passthrough': [False, True],\n",
    "    'cv': [5],\n",
    "    'lr__C': [0.1, 1.0],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'gb__n_estimators': [100, 200],\n",
    "    'gb__learning_rate': [0.1, 0.2],\n",
    "    'xgb__learning_rate': [0.05, 0.1],\n",
    "    'final_estimator__C': [1.0, 10]\n",
    "}\n",
    "\n",
    "# Grid 6 & 7: SMOTE pipelines - Handle imbalanced data\n",
    "param_grid_smote = {\n",
    "    'model__passthrough': [False],\n",
    "    'model__cv': [5],\n",
    "    'model__rf__n_estimators': [100, 200],\n",
    "    'model__rf__max_depth': [10, 20],\n",
    "    'model__gb__n_estimators': [100, 200],\n",
    "    'model__xgb__max_depth': [3, 5],\n",
    "    'model__final_estimator__C': [1.0, 10]\n",
    "}\n",
    "\n",
    "# Grid 8-11: Undersampling and Combined pipelines\n",
    "param_grid_sampling = {\n",
    "    'model__passthrough': [False],\n",
    "    'model__cv': [5],\n",
    "    'model__rf__n_estimators': [100, 200],\n",
    "    'model__gb__learning_rate': [0.05, 0.1],\n",
    "    'model__xgb__n_estimators': [100, 200],\n",
    "    'model__final_estimator__C': [1.0, 10]\n",
    "}\n",
    "\n",
    "# Map grids to configurations\n",
    "hyperparameter_grids = {\n",
    "    'Preprocessed Data': param_grid_1,\n",
    "    'Normalized Data': param_grid_2,\n",
    "    'SelectKBest': param_grid_3,\n",
    "    'RFECV': param_grid_4,\n",
    "    'PCA': param_grid_5,\n",
    "    'SMOTE + StandardScaler': param_grid_smote,\n",
    "    'SMOTE + GridSearchCV': param_grid_smote,\n",
    "    'RandomUnderSampler': param_grid_sampling,\n",
    "    'TomekLinks': param_grid_sampling,\n",
    "    'NearMiss': param_grid_sampling,\n",
    "    'SMOTE + Tomek': param_grid_sampling,\n",
    "}\n",
    "\n",
    "# Initialize configurations list\n",
    "configurations = []\n",
    "\n",
    "# --- Configuration 1: Preprocessed Data ---\n",
    "configurations.append(('Preprocessed Data', 'array', X_train_preprocessed, X_val_preprocessed))\n",
    "print(\"‚úì Configuration 1: Preprocessed Data\")\n",
    "\n",
    "# --- Configuration 2: Normalized Data (MinMax on preprocessed) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_normalized = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_preprocessed), \n",
    "    columns=X_train_preprocessed.columns, \n",
    "    index=X_train_preprocessed.index\n",
    ")\n",
    "X_val_normalized = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_val_preprocessed), \n",
    "    columns=X_val_preprocessed.columns, \n",
    "    index=X_val_preprocessed.index\n",
    ")\n",
    "configurations.append(('Normalized Data', 'array', X_train_normalized, X_val_normalized))\n",
    "print(\"‚úì Configuration 2: Normalized Data (MinMax)\")\n",
    "\n",
    "# --- Configuration 3: SelectKBest ---\n",
    "print(\"\\n=== SelectKBest Feature Selection ===\")\n",
    "scores = []\n",
    "max_features = min(X_train_normalized.shape[1], 20)\n",
    "\n",
    "# Create base estimators for stacking\n",
    "lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "gb = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "xgb = XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "\n",
    "for k in range(1, max_features + 1):\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_tr_k = kbest.fit_transform(X_train_normalized, y_train)\n",
    "    \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=[('lr', lr), ('rf', rf), ('gb', gb), ('xgb', xgb)],\n",
    "        final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(\n",
    "        stacking_clf, X_tr_k, y_train, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "optimal_k = int(np.argmax(scores) + 1)\n",
    "print(f\"Optimal number of features: {optimal_k}\")\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "X_train_kbest = pd.DataFrame(\n",
    "    kbest.fit_transform(X_train_normalized, y_train), \n",
    "    columns=X_train_normalized.columns[kbest.get_support()]\n",
    ")\n",
    "X_val_kbest = pd.DataFrame(\n",
    "    kbest.transform(X_val_normalized), \n",
    "    columns=X_train_kbest.columns\n",
    ")\n",
    "configurations.append(('SelectKBest', 'array', X_train_kbest, X_val_kbest))\n",
    "print(\"‚úì Configuration 3: SelectKBest\")\n",
    "\n",
    "# --- Configuration 4: RFECV ---\n",
    "print(\"\\n=== RFECV Feature Selection ===\")\n",
    "stacking_estimator = StackingClassifier(\n",
    "    estimators=[('lr', lr), ('rf', rf), ('gb', gb), ('xgb', xgb)],\n",
    "    final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    cv=5\n",
    ")\n",
    "rfecv = RFECV(\n",
    "    estimator=stacking_estimator, \n",
    "    step=1, \n",
    "    cv=StratifiedKFold(5), \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train_kbest, y_train)\n",
    "print(f\"Optimal number of features by RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "rfe = RFE(estimator=stacking_estimator, n_features_to_select=rfecv.n_features_)\n",
    "X_train_rfe = pd.DataFrame(\n",
    "    rfe.fit_transform(X_train_kbest, y_train), \n",
    "    columns=X_train_kbest.columns[rfe.get_support()]\n",
    ")\n",
    "X_val_rfe = pd.DataFrame(\n",
    "    rfe.transform(X_val_kbest), \n",
    "    columns=X_train_rfe.columns\n",
    ")\n",
    "configurations.append(('RFECV', 'array', X_train_rfe, X_val_rfe))\n",
    "print(\"‚úì Configuration 4: RFECV\")\n",
    "\n",
    "# --- Configuration 5: PCA ---\n",
    "print(\"\\n=== PCA Dimensionality Reduction ===\")\n",
    "pca_full = PCA().fit(X_train_rfe)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "desired_variance = 0.90\n",
    "n_components = int(np.argmax(cumulative_variance >= desired_variance) + 1)\n",
    "n_components = max(2, n_components)\n",
    "print(f'Number of components for {desired_variance * 100}% variance: {n_components}')\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_rfe), index=X_train_rfe.index)\n",
    "X_val_pca = pd.DataFrame(pca.transform(X_val_rfe), index=X_val_rfe.index)\n",
    "configurations.append(('PCA', 'array', X_train_pca, X_val_pca))\n",
    "print(\"‚úì Configuration 5: PCA\")\n",
    "\n",
    "# --- Configuration 6: SMOTE + StandardScaler (Pipeline) ---\n",
    "pipeline_smote_scaler = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "configurations.append(('SMOTE + StandardScaler', 'pipeline', pipeline_smote_scaler, None))\n",
    "print(\"‚úì Configuration 6: SMOTE + StandardScaler (Pipeline)\")\n",
    "\n",
    "# --- Configuration 7: SMOTE + GridSearchCV (Pipeline) ---\n",
    "pipeline_smote_grid = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('model', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "configurations.append(('SMOTE + GridSearchCV', 'pipeline', pipeline_smote_grid, None))\n",
    "print(\"‚úì Configuration 7: SMOTE + GridSearchCV (Pipeline)\")\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSAMPLING CONFIGURATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== Adding Undersampling Configurations ===\")\n",
    "\n",
    "# --- Configuration 8: RandomUnderSampler ---\n",
    "pipeline_rus = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "configurations.append(('RandomUnderSampler', 'pipeline', pipeline_rus, None))\n",
    "print(\"‚úì Configuration 8: RandomUnderSampler (Undersampling)\")\n",
    "\n",
    "# --- Configuration 9: TomekLinks ---\n",
    "pipeline_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', TomekLinks()),\n",
    "    ('model', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "configurations.append(('TomekLinks', 'pipeline', pipeline_tomek, None))\n",
    "print(\"‚úì Configuration 9: TomekLinks (Undersampling - removes noisy samples)\")\n",
    "\n",
    "# --- Configuration 10: NearMiss ---\n",
    "pipeline_nearmiss = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('undersampler', NearMiss(version=1)),\n",
    "    ('model', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "configurations.append(('NearMiss', 'pipeline', pipeline_nearmiss, None))\n",
    "print(\"‚úì Configuration 10: NearMiss (Undersampling - selective)\")\n",
    "\n",
    "# --- Configuration 11: SMOTE + Tomek (Combined) ---\n",
    "pipeline_smote_tomek = ImbPipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])),\n",
    "    ('sampler', SMOTETomek(random_state=RANDOM_STATE)),\n",
    "    ('model', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "            ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "            ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "configurations.append(('SMOTE + Tomek', 'pipeline', pipeline_smote_tomek, None))\n",
    "print(\"‚úì Configuration 11: SMOTE + Tomek (Combined Over + Under)\")\n",
    "\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "\n",
    "# Safe ROC AUC helper\n",
    "def safe_roc_auc(y_true, y_proba):\n",
    "    try:\n",
    "        if isinstance(y_proba, np.ndarray) and y_proba.shape[1] == 2:\n",
    "            return metrics.roc_auc_score(y_true, y_proba[:, 1])\n",
    "        else:\n",
    "            return metrics.roc_auc_score(\n",
    "                pd.get_dummies(y_true), y_proba, \n",
    "                multi_class='ovr', average='macro'\n",
    "            )\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Run Stacking Classifier with Configuration-Specific GridSearchCV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING STACKING CLASSIFIER WITH CONFIGURATION-SPECIFIC HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kind, X_tr_cfg, X_val_cfg in configurations:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get the specific parameter grid for this configuration\n",
    "    param_grid = hyperparameter_grids[name]\n",
    "    \n",
    "    print(f\"Hyperparameter grid for '{name}':\")\n",
    "    for key, values in param_grid.items():\n",
    "        print(f\"  {key}: {values}\")\n",
    "    print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    \n",
    "    try:\n",
    "        if kind == 'pipeline':\n",
    "            pipeline = X_tr_cfg\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_train_proba = best_model.predict_proba(X_train)\n",
    "            y_val_proba = best_model.predict_proba(X_val)\n",
    "        else:\n",
    "            # Create stacking classifier with base estimators and meta-learner\n",
    "            stacking_clf = StackingClassifier(\n",
    "                estimators=[\n",
    "                    ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)),\n",
    "                    ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "                    ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "                    ('xgb', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n",
    "                ],\n",
    "                final_estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "                cv=5\n",
    "            )\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                stacking_clf, param_grid, \n",
    "                cv=5, n_jobs=-1, verbose=1, scoring='f1_macro'\n",
    "            )\n",
    "            grid_search.fit(X_tr_cfg, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_tr_cfg)\n",
    "            y_val_pred = best_model.predict(X_val_cfg)\n",
    "            y_train_proba = best_model.predict_proba(X_tr_cfg)\n",
    "            y_val_proba = best_model.predict_proba(X_val_cfg)\n",
    "\n",
    "        # Calculate train-test gap for overfitting detection\n",
    "        train_acc = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        train_test_gap = train_acc - test_acc\n",
    "\n",
    "        # Build metrics dictionary\n",
    "        metrics_dict = {\n",
    "            \"Dataset\": [\"Training\", \"Test\"],\n",
    "            \"Accuracy\": [train_acc, test_acc],\n",
    "            \"F1 Score\": [\n",
    "                metrics.f1_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.f1_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Recall\": [\n",
    "                metrics.recall_score(y_train, y_train_pred, average='macro'),\n",
    "                metrics.recall_score(y_val, y_val_pred, average='macro'),\n",
    "            ],\n",
    "            \"Precision\": [\n",
    "                metrics.precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
    "                metrics.precision_score(y_val, y_val_pred, average='macro', zero_division=0),\n",
    "            ],\n",
    "            \"AUC-ROC\": [\n",
    "                safe_roc_auc(y_train, y_train_proba),\n",
    "                safe_roc_auc(y_val, y_val_proba),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_metrics = pd.DataFrame(metrics_dict)\n",
    "        pd.options.display.float_format = '{:.6f}'.format\n",
    "        print(\"\\nüìä Stacking Classifier Model Performance Metrics\")\n",
    "        print(df_metrics.to_string(index=False))\n",
    "\n",
    "        # Overfitting warning\n",
    "        if train_test_gap > 0.10:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Overfitting detected! Train-Test gap: {train_test_gap:.4f}\")\n",
    "            print(\"   Consider: adjusting base estimators, meta-learner parameters, or CV folds\")\n",
    "        elif train_test_gap < 0.05:\n",
    "            print(f\"\\n‚úì Excellent generalization. Train-Test gap: {train_test_gap:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚Üí Acceptable gap: {train_test_gap:.4f}\")\n",
    "\n",
    "        # Store test metrics\n",
    "        test_metrics = df_metrics[df_metrics['Dataset'] == 'Test'].iloc[0]\n",
    "        try:\n",
    "            store_results(\n",
    "                'Stacking Classifier',\n",
    "                name,\n",
    "                float(test_metrics['Accuracy']),\n",
    "                float(test_metrics['F1 Score']),\n",
    "                float(test_metrics['Recall']),\n",
    "                float(test_metrics['Precision']),\n",
    "                float(test_metrics['AUC-ROC'])\n",
    "            )\n",
    "        except:\n",
    "            ML_Model.append('Stacking Classifier')\n",
    "            ML_Config.append(name)\n",
    "            accuracy.append(round(float(test_metrics['Accuracy']), 6))\n",
    "            f1.append(round(float(test_metrics['F1 Score']), 6))\n",
    "            recall.append(round(float(test_metrics['Recall']), 6))\n",
    "            precision.append(round(float(test_metrics['Precision']), 6))\n",
    "            roc_auc.append(round(float(test_metrics['AUC-ROC']), 6))\n",
    "\n",
    "        print(\"\\nüéØ Best hyperparameters found:\")\n",
    "        best_params = grid_search.best_params_\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest CV score: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Display stacking details\n",
    "        print(f\"\\nüìö Stacking Ensemble Details:\")\n",
    "        print(f\"  CV Folds: {best_model.cv}\")\n",
    "        print(f\"  Passthrough: {best_model.passthrough}\")\n",
    "        print(f\"  Number of Base Estimators: {len(best_model.estimators_)}\")\n",
    "        print(f\"  Base Estimators: {[name for name, _ in best_model.estimators]}\")\n",
    "        print(f\"  Meta-Learner: {type(best_model.final_estimator_).__name__}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in configuration '{name}': {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Stacking Classifier evaluation complete for all configurations.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display final results\n",
    "try:\n",
    "    display_and_save_results('stacking_classifier_all_configs')\n",
    "except:\n",
    "    result = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    print(result.to_string(index=False))\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    sorted_result = result.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nüèÜ Sorted Results (by F1 Score):\")\n",
    "    print(sorted_result.to_string(index=False))\n",
    "\n",
    "    # Group by sampling technique\n",
    "    print(\"\\nüìä Performance by Sampling Technique:\")\n",
    "    print(\"-\" * 80)\n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = result[result['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{group_name}:\")\n",
    "            print(f\"  Avg Accuracy: {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score: {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC: {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            print(f\"  Best Config: {group_data.loc[group_data['F1 Score'].idxmax(), 'Configuration']}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if len(result) > 0:\n",
    "        best_idx = result['F1 Score'].idxmax()\n",
    "        print(f\"\\nüèÖ Best Overall Configuration:\")\n",
    "        print(f\"  Configuration: {result.loc[best_idx, 'Configuration']}\")\n",
    "        print(f\"  F1 Score: {result.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {result.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "        print(f\"  ROC-AUC: {result.loc[best_idx, 'ROC_AUC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78501f2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0264b",
   "metadata": {},
   "source": [
    "### Bagging classification with PCA 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SelectKBest Feature Selection ===\n",
      "Optimal number of features to select using SelectKBest: 22\n",
      "\n",
      "=== RFECV Feature Selection with Bagging ===\n",
      "Optimal number of features selected by RFECV: 1\n",
      "\n",
      "=== PCA Dimensionality Reduction ===\n",
      "Number of components that explain 90.0% variance: 1\n",
      "\n",
      "=== Bagging Model Performance with Hyperparameter Tuning ===\n",
      "\n",
      "Running Bagging with Original Data configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.981013  0.916868 0.857639   0.993197 0.999923\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.954901\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 0.8, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with Normalized Data configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.981013  0.916868 0.857639   0.993197 0.999849\n",
      "    Test  0.933649  0.599386 0.530303   0.977346 0.949225\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with SelectKBest configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.979430  0.907726 0.843750   0.992643 0.999750\n",
      "    Test  0.943128  0.688312 0.590909   0.980392 0.937465\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(min_samples_split=10), 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n",
      "\n",
      "Running Bagging with RFECV configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.911392  0.317881 0.333333   0.303797 0.705290\n",
      "    Test  0.909953  0.317618 0.333333   0.303318 0.709804\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=3), 'max_features': 0.6, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "\n",
      "Running Bagging with PCA configuration...\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "\n",
      "Bagging Model Performance Metrics\n",
      " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
      "Training  0.911392  0.317881 0.333333   0.303797 0.706551\n",
      "    Test  0.909953  0.317618 0.333333   0.303318 0.713093\n",
      "Best hyperparameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'bootstrap_features': False, 'estimator': DecisionTreeClassifier(max_depth=3), 'max_features': 0.6, 'max_samples': 0.6, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Bagging Classifier with Configuration-Specific Hyperparameter Grids\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, RFECV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define preprocessor for categorical and numeric features\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to get fully numeric data first\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names = (numeric_features + \n",
    "                    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
    "except:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train_preprocessed.shape[1])]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=feature_names, index=X_train.index)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_val.index)\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"All features are now numeric: {X_train_preprocessed.select_dtypes(include=np.number).shape[1] == X_train_preprocessed.shape[1]}\")\n",
    "\n",
    "# Clear previous results\n",
    "try:\n",
    "    clear_results()\n",
    "except:\n",
    "    ML_Model = []\n",
    "    ML_Config = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    roc_auc = []\n",
    "    print(\"Initialized result storage lists.\")\n",
    "\n",
    "print(\"\\n=== START: Bagging Classifier Configuration Sweep with Custom Hyperparameters ===\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration-Specific Hyperparameter Grids for Bagging Classifier\n",
    "# =============================================================================\n",
    "\n",
    "# Grid 1: Preprocessed Data - Explore bootstrap and sampling\n",
    "param_grid_1 = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_samples': [0.7, 0.8, 1.0],\n",
    "    'max_features': [0.7, 0.8, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'bootstrap_features': [False],\n",
    "    'estimator__max_depth': [5, 10, 15],\n",
    "    'estimator__min_samples_split': [2, 5],\n",
    "    'estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd3688",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc80be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Results storage framework not initialized\n",
      "Please run the model evaluation cells first, or load saved results from CSV\n",
      "\n",
      "‚ùå Results directory not found: Analysis/Main/results\n",
      "\n",
      "‚ùå No results available. Please run model evaluations first.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE RESULTS ANALYSIS - ALL MODELS\n",
    "# =============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Check if results storage framework variables exist\n",
    "try:\n",
    "    # Try to access the variables\n",
    "    test = ML_Model\n",
    "    print(\"‚úì Results storage framework is active\")\n",
    "    print(f\"‚úì Total entries in storage: {len(ML_Model)}\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è  Results storage framework not initialized\")\n",
    "    print(\"Please run the model evaluation cells first, or load saved results from CSV\")\n",
    "    \n",
    "    # Try to load from saved CSV files\n",
    "    results_dir = 'Analysis/Main/results'\n",
    "    all_csv_files = []\n",
    "    \n",
    "    if os.path.exists(results_dir):\n",
    "        # Find all result CSV files\n",
    "        for file in os.listdir(results_dir):\n",
    "            if file.endswith('_results.csv') and not file.startswith('sorted'):\n",
    "                all_csv_files.append(os.path.join(results_dir, file))\n",
    "        \n",
    "        if all_csv_files:\n",
    "            print(f\"\\n‚úì Found {len(all_csv_files)} saved result files\")\n",
    "            # Load and combine all results\n",
    "            dfs = []\n",
    "            for csv_file in all_csv_files:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                dfs.append(df)\n",
    "                print(f\"  - Loaded: {os.path.basename(csv_file)} ({len(df)} rows)\")\n",
    "            \n",
    "            all_results = pd.concat(dfs, ignore_index=True)\n",
    "            print(f\"\\n‚úì Combined results: {len(all_results)} total configurations\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå No saved result files found in\", results_dir)\n",
    "            all_results = pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Results directory not found: {results_dir}\")\n",
    "        all_results = pd.DataFrame()\n",
    "\n",
    "# If variables exist, compile from storage\n",
    "if 'ML_Model' in globals() and len(ML_Model) > 0:\n",
    "    all_results = pd.DataFrame({\n",
    "        'ML Model': ML_Model,\n",
    "        'Configuration': ML_Config,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'ROC_AUC': roc_auc,\n",
    "    })\n",
    "    \n",
    "    # Remove duplicates\n",
    "    all_results.drop_duplicates(subset=['ML Model', 'Configuration'], inplace=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"üìà COMPREHENSIVE RESULTS - ALL MACHINE LEARNING MODELS\")\n",
    "    print(\"=\"*120)\n",
    "    print(all_results.to_string(index=False))\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    results_dir = 'Analysis/Main/results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    save_path = os.path.join(results_dir, 'all_models_comprehensive_results.csv')\n",
    "    all_results.to_csv(save_path, index=False)\n",
    "    print(f\"\\n‚úì Comprehensive results saved to {save_path}\")\n",
    "    \n",
    "    # Sort by F1 Score and Accuracy\n",
    "    sorted_results = all_results.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"üèÜ SORTED COMPREHENSIVE RESULTS (by F1 Score & Accuracy)\")\n",
    "    print(\"=\"*120)\n",
    "    print(sorted_results.to_string(index=False))\n",
    "    \n",
    "    # Save sorted results\n",
    "    sorted_save_path = os.path.join(results_dir, 'all_models_sorted_results.csv')\n",
    "    sorted_results.to_csv(sorted_save_path, index=False)\n",
    "    print(f\"\\n‚úì Sorted results saved to {sorted_save_path}\")\n",
    "\n",
    "elif len(all_results) > 0:\n",
    "    # Working with loaded CSV data\n",
    "    sorted_results = all_results.sort_values(by=['F1 Score', 'Accuracy'], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"üìà LOADED RESULTS - ALL MACHINE LEARNING MODELS\")\n",
    "    print(\"=\"*120)\n",
    "    print(all_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"üèÜ SORTED RESULTS (by F1 Score & Accuracy)\")\n",
    "    print(\"=\"*120)\n",
    "    print(sorted_results.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚ùå No results available. Please run model evaluations first.\")\n",
    "    sorted_results = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720aa104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  No results to display. Please run model evaluations first or check if CSV files exist.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED ANALYSIS - ALL MODELS\n",
    "# =============================================================================\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. GROUP BY SAMPLING TECHNIQUE\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"üìä PERFORMANCE BY SAMPLING TECHNIQUE (ALL MODELS)\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    sampling_groups = {\n",
    "        'No Sampling': ['Preprocessed Data', 'Normalized Data', 'SelectKBest', 'RFECV', 'PCA'],\n",
    "        'Oversampling': ['SMOTE + StandardScaler', 'SMOTE + GridSearchCV'],\n",
    "        'Undersampling': ['RandomUnderSampler', 'TomekLinks', 'NearMiss'],\n",
    "        'Combined': ['SMOTE + Tomek']\n",
    "    }\n",
    "    \n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = all_results[all_results['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            print(f\"\\n{'‚îÄ'*60}\")\n",
    "            print(f\"üîπ {group_name}\")\n",
    "            print(f\"{'‚îÄ'*60}\")\n",
    "            print(f\"  Configurations: {len(group_data)}\")\n",
    "            print(f\"  Avg Accuracy:    {group_data['Accuracy'].mean():.6f}\")\n",
    "            print(f\"  Avg F1 Score:    {group_data['F1 Score'].mean():.6f}\")\n",
    "            print(f\"  Avg Recall:      {group_data['Recall'].mean():.6f}\")\n",
    "            print(f\"  Avg Precision:   {group_data['Precision'].mean():.6f}\")\n",
    "            print(f\"  Avg ROC-AUC:     {group_data['ROC_AUC'].mean():.6f}\")\n",
    "            best_in_group = group_data.loc[group_data['F1 Score'].idxmax()]\n",
    "            print(f\"  Best Model:      {best_in_group['ML Model']}\")\n",
    "            print(f\"  Best Config:     {best_in_group['Configuration']}\")\n",
    "            print(f\"  Best F1 Score:   {best_in_group['F1 Score']:.6f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. TOP CONFIGURATION PER MODEL\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"ü•á TOP CONFIGURATION PER MODEL\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    top_per_model = sorted_results.groupby('ML Model', as_index=False).first()\n",
    "    print(top_per_model.to_string(index=False))\n",
    "    \n",
    "    # Save top configurations\n",
    "    results_dir = 'Analysis/Main/results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    top_save_path = os.path.join(results_dir, 'top_configuration_per_model.csv')\n",
    "    top_per_model.to_csv(top_save_path, index=False)\n",
    "    print(f\"\\n‚úì Top configurations saved to {top_save_path}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. BEST OVERALL CONFIGURATION\n",
    "    # =========================================================================\n",
    "    best_idx = all_results['F1 Score'].idxmax()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"üèÖ BEST OVERALL CONFIGURATION ACROSS ALL MODELS\")\n",
    "    print(\"=\"*120)\n",
    "    print(f\"  Model:           {all_results.loc[best_idx, 'ML Model']}\")\n",
    "    print(f\"  Configuration:   {all_results.loc[best_idx, 'Configuration']}\")\n",
    "    print(f\"  {'‚îÄ'*70}\")\n",
    "    print(f\"  ‚úì Accuracy:      {all_results.loc[best_idx, 'Accuracy']:.6f}\")\n",
    "    print(f\"  ‚úì F1 Score:      {all_results.loc[best_idx, 'F1 Score']:.6f}\")\n",
    "    print(f\"  ‚úì Recall:        {all_results.loc[best_idx, 'Recall']:.6f}\")\n",
    "    print(f\"  ‚úì Precision:     {all_results.loc[best_idx, 'Precision']:.6f}\")\n",
    "    print(f\"  ‚úì ROC-AUC:       {all_results.loc[best_idx, 'ROC_AUC']:.6f}\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. MODEL COMPARISON\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"üîç MODEL-BY-MODEL COMPARISON (AVERAGE PERFORMANCE)\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    model_comparison = all_results.groupby('ML Model').agg({\n",
    "        'Accuracy': 'mean',\n",
    "        'F1 Score': 'mean',\n",
    "        'Recall': 'mean',\n",
    "        'Precision': 'mean',\n",
    "        'ROC_AUC': 'mean'\n",
    "    }).round(6).sort_values('F1 Score', ascending=False)\n",
    "    \n",
    "    print(model_comparison.to_string())\n",
    "    \n",
    "    # Save model comparison\n",
    "    comparison_save_path = os.path.join(results_dir, 'model_comparison_averages.csv')\n",
    "    model_comparison.to_csv(comparison_save_path)\n",
    "    print(f\"\\n‚úì Model comparison saved to {comparison_save_path}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. KEY INSIGHTS & STATISTICS\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"üìå KEY INSIGHTS & STATISTICS\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    print(f\"\\n  üìä Overall Statistics:\")\n",
    "    print(f\"     ‚Ä¢ Total models evaluated: {all_results['ML Model'].nunique()}\")\n",
    "    print(f\"     ‚Ä¢ Total configurations tested: {len(all_results)}\")\n",
    "    print(f\"     ‚Ä¢ Avg tests per model: {len(all_results) / all_results['ML Model'].nunique():.1f}\")\n",
    "    \n",
    "    print(f\"\\n  üéØ Performance Thresholds:\")\n",
    "    print(f\"     ‚Ä¢ Configurations with F1 > 0.95: {len(all_results[all_results['F1 Score'] > 0.95])}\")\n",
    "    print(f\"     ‚Ä¢ Configurations with F1 > 0.97: {len(all_results[all_results['F1 Score'] > 0.97])}\")\n",
    "    print(f\"     ‚Ä¢ Configurations with Accuracy > 0.98: {len(all_results[all_results['Accuracy'] > 0.98])}\")\n",
    "    print(f\"     ‚Ä¢ Configurations with ROC-AUC > 0.99: {len(all_results[all_results['ROC_AUC'] > 0.99])}\")\n",
    "    \n",
    "    print(f\"\\n  üìà Performance Ranges:\")\n",
    "    print(f\"     ‚Ä¢ F1 Score:   [{all_results['F1 Score'].min():.6f} - {all_results['F1 Score'].max():.6f}] (Œî {all_results['F1 Score'].max() - all_results['F1 Score'].min():.6f})\")\n",
    "    print(f\"     ‚Ä¢ Accuracy:   [{all_results['Accuracy'].min():.6f} - {all_results['Accuracy'].max():.6f}] (Œî {all_results['Accuracy'].max() - all_results['Accuracy'].min():.6f})\")\n",
    "    print(f\"     ‚Ä¢ ROC-AUC:    [{all_results['ROC_AUC'].min():.6f} - {all_results['ROC_AUC'].max():.6f}] (Œî {all_results['ROC_AUC'].max() - all_results['ROC_AUC'].min():.6f})\")\n",
    "    \n",
    "    print(f\"\\n  üìâ Variability (Standard Deviation):\")\n",
    "    print(f\"     ‚Ä¢ F1 Score:   {all_results['F1 Score'].std():.6f}\")\n",
    "    print(f\"     ‚Ä¢ Accuracy:   {all_results['Accuracy'].std():.6f}\")\n",
    "    print(f\"     ‚Ä¢ ROC-AUC:    {all_results['ROC_AUC'].std():.6f}\")\n",
    "    \n",
    "    print(f\"\\n  üèÜ Best Performing Models:\")\n",
    "    top_3 = sorted_results.head(3)\n",
    "    for i, (_, row) in enumerate(top_3.iterrows(), 1):\n",
    "        print(f\"     {i}. {row['ML Model']} ({row['Configuration']}): F1={row['F1 Score']:.6f}\")\n",
    "    \n",
    "    print(f\"\\n  üí° Best Sampling Technique:\")\n",
    "    best_sampling = None\n",
    "    best_sampling_f1 = 0\n",
    "    for group_name, configs in sampling_groups.items():\n",
    "        group_data = all_results[all_results['Configuration'].isin(configs)]\n",
    "        if not group_data.empty:\n",
    "            avg_f1 = group_data['F1 Score'].mean()\n",
    "            if avg_f1 > best_sampling_f1:\n",
    "                best_sampling_f1 = avg_f1\n",
    "                best_sampling = group_name\n",
    "    if best_sampling:\n",
    "        print(f\"     ‚Ä¢ {best_sampling} (Avg F1: {best_sampling_f1:.6f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"‚úÖ COMPREHENSIVE MODEL EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*120)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No results to display. Please run model evaluations first or check if CSV files exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143cf9c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
